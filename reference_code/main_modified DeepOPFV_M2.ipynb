{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976deb84-7de2-4250-9390-fb947bae7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepOPF-Dis: distributed DeepOPF-V\n",
    "# 2024-3-18\n",
    "# 2024-3-24: no constraint penalty and no kron reduction\n",
    "# 2024-3-25: no constraint penalty but add kron reduction\n",
    "# 2024-4-6: use multi-branch DNN supervised learning\n",
    "# 2024-4-6: add penalty \n",
    "# 2024-4-7: simplify the code\n",
    "# 2024-4-10: single branch DNN\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math \n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Function\n",
    "from scipy import sparse\n",
    "from torch.nn.utils import weight_norm as wn\n",
    "import gc\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# from torchviz import make_dot\n",
    "random.seed(12343)\n",
    "\n",
    "global BRANFT, branch,finc,tinc,Nbranch \n",
    "global Ybus,baseMVA,baseMVA_tensor\n",
    "global Vscale, Vbias, bus_Pg, bus_Qg, bus_Pd, bus_Qd, bus_Pnet_nonPg, bus_Pnet_nonQg, bus_Va\n",
    "global flagV,DELTA,Nbus,Ntest,Ng,Nd\n",
    "global idxPg_tensor, gencost_tensor\n",
    "global kcost, kpd, kqd, kgenp, kgenq, kbpf, kbang, kv\n",
    "global MAXMIN_PQg, MAXMIN_Pg_tensor, MAXMIN_Qg_tensor, MAXMIN_PQgall_tensor\n",
    "global MAXMIN_Pg, MAXMIN_Qg, MAXMIN_PQgall\n",
    "global kpd_max,kqd_max,kgenp_max,kgenq_max, kv_max\n",
    "global flag_k \n",
    "global idx_bus_Pnet_Vam_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f27b33-73b0-45a1-9be3-08d4c9c62375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n",
      "Let's use 1 GPUs!\n",
      "fignm ./result/ls_DeepOPFSB_krp_B30r0V1S32000Nhis3lr0.0001kc0.0kd100kg2000kv500ky1L32224bs50E4500.jpg\n",
      "***************begin case 30 sys_R 0 ***************\n",
      "kcoef B30r0V1S32000Nhis3lr0.0001kc0.0kd100kg2000kv500ky1L32224bs50 \n",
      "kcoef B30r0V1S32000Nhis3lr0.0001kc0.0kd100kg2000kv500ky1L32224bs50 \n",
      "PATHs ./model/DeepOPFSB_krp_B30r0V1S32000Nhis3lr0.0001kc0.0kd100kg2000kv500ky1L32224bs50 \n",
      "PATH ./model/DeepOPFSB_krp_B30r0V1S32000Nhis3lr0.0001kc0.0kd100kg2000kv500ky1L32224bs50E4500.pth \n",
      "resultnm ./result/Res_DeepOPFSB_krp_B30r0V1S32000Nhis3lr0.0001kc0.0kd100kg2000kv500ky1L32224bs50E45001Nt10000.mat\n",
      "training setting:  T 1 Ntrain 32000 Ntest 10000 Nhis 3 batch_size 50  Lr 0.0001  Epoch 4500\n",
      "kcost tensor([0.0002]) kpd tensor([100.]) kqd tensor([100.]) kgenp tensor([2000.]) kgenq tensor([2000.]) k_dV 0.1\n"
     ]
    }
   ],
   "source": [
    "## print('*'*10,'L2 sigmoid','*'*10)\n",
    "\n",
    "## whether there is GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device',device)\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    \n",
    "# 控制打印格式保留小数点n位\n",
    "torch.set_printoptions(precision=4)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "## load system name\n",
    "version = 1 # number of model\n",
    "T = 1 # running the Tth time\n",
    "# Nbus = 118\n",
    "# sys_R = 4 # case300:[0, 1, 2]; case118[0, 1]; case30[0, 1]\n",
    "\n",
    "Nbus = 30\n",
    "sys_R = 0 # case300:[0, 1, 2]; case118[0, 1]; case30[0, 1]\n",
    "\n",
    "\n",
    "flag_test = 1 # test\n",
    "flag_save_ls = 0 # save values of loss function\n",
    "s_epoch = 3000 # save trainged model during training\n",
    "p_epoch = 100 # print loss function value\n",
    "Epoch = 4500\n",
    "Nsample = 50000 # number of samples: including training, test and post-processing samples\n",
    "Nhis = 3 # number of sampels used for post-processing\n",
    "Ntrain = 32000\n",
    "Ntest = 10000\n",
    "batch_size_training = 50\n",
    "batch_size_test = 1\n",
    "\n",
    "## DNN structure: fully connected layers\n",
    "hidden_units = 1\n",
    "# khidden = np.array([256,336], dtype=int)\n",
    "khidden = np.array([32,224], dtype=int)\n",
    "# khidden = np.array([256,128], dtype=int)\n",
    "\n",
    "nmL = 'L'\n",
    "for i in range(khidden.shape[0]):\n",
    "    nmL = nmL + str(khidden[i])  \n",
    "\n",
    "Lr = 1e-4\n",
    "DELTA = 1e-4 # threshold of violation\n",
    "kdelta = 1e-4 # threshold of violation in training\n",
    "    \n",
    "\n",
    "## loss function coefficients\n",
    "flag_k = 1\n",
    "\n",
    "kpy = 1 # penalty for constraint violation\n",
    "if kpy > 0 :\n",
    "    method = '_krp' # kron reduction & penlaty\n",
    "else:\n",
    "    method = '_krngtp' # kron reduction & penlaty\n",
    "\n",
    "kpy = torch.tensor([kpy])\n",
    "    \n",
    "kcost = torch.tensor([0.0002]) # scaling of Pg cost\n",
    "Ki = torch.tensor([100.0]) # tensor coefficent\n",
    "# Ki = torch.tensor([1.0]) # tensor coefficent\n",
    "kpdi = 100.0 # P load\n",
    "kqdi = 100.0 # Q load\n",
    "kgenpi = 2000.0 # Pg\n",
    "kgenqi = 2000.0 # Qg\n",
    "kvi = 100.0 # voltage violation for recovered ZIBs \n",
    "k_dV = 0.1 # post-processing coefficient: step of delta_Va delta_Vm\n",
    "\n",
    "# upper bound of loss function coefficients\n",
    "kpd_max = torch.tensor([100.0])\n",
    "kqd_max = torch.tensor([100.0])\n",
    "kgenp_max = torch.tensor([2000.0])\n",
    "kgenq_max = torch.tensor([2000.0])\n",
    "kv_max = torch.tensor([500.0])\n",
    "\n",
    "# save loss function value during training  \n",
    "ls_cost_log = []\n",
    "ls_Pg_log = []\n",
    "ls_Qg_log = []\n",
    "ls_Pd_log = []\n",
    "ls_Qd_log = []\n",
    "\n",
    "k_cost_log = []\n",
    "k_Pg_log = []\n",
    "k_Qg_log = []\n",
    "k_Pd_log = []\n",
    "k_Qd_log = []\n",
    "\n",
    "# file names\n",
    "kcoef = 'B'+str(Nbus)+'r'+str(sys_R)+'V'+str(version)+'S'+str(int(Ntrain))+'Nhis'+str(int(Nhis))+'lr'+str(Lr)+'kc'+str(round(float(kcost),2))+'kd'+str(int(kpd_max))+\\\n",
    "'kg'+str(int(kgenp_max))+'kv'+str(int(kv_max))+'ky'+str(int(kpy))+nmL+'bs'+str(batch_size_training)\n",
    "# path name of trained models\n",
    "PATH = './model/DeepOPFSB'+method+'_'+kcoef+'E'+str(Epoch)+'.pth'\n",
    "PATHs = './model/DeepOPFSB'+method+'_'+kcoef\n",
    "resultnm =  './result/Res_DeepOPFSB'+method+'_'+kcoef+'E'+str(Epoch)+str(T)+'Nt'+str(Ntest)+'.mat'\n",
    "fignm = './result/ls_DeepOPFSB'+method+'_'+kcoef+'E'+str(Epoch)+'.jpg'\n",
    "print('fignm', fignm)\n",
    "\n",
    "print('***************begin case',Nbus,'sys_R',sys_R,'***************')\n",
    "print('kcoef',kcoef,'\\nkcoef',kcoef,'\\nPATHs',PATHs,'\\nPATH',PATH,'\\nresultnm',resultnm)\n",
    "print('training setting: ','T', T,'Ntrain',Ntrain,'Ntest',Ntest,'Nhis', Nhis,'batch_size',batch_size_training, \\\n",
    "      ' Lr', Lr,' Epoch',Epoch)\n",
    "print('kcost', kcost,'kpd',kpd_max,'kqd',kqd_max,'kgenp',kgenp_max,'kgenq',kgenq_max,'k_dV',k_dV)\n",
    "\n",
    "# load data\n",
    "if Nbus == 30:\n",
    "    # B30\n",
    "    matnm = './data/XY_case30_real_Nsample5W_ipopt.mat' \n",
    "    matparanm = './data/case30_para.mat'\n",
    "    matcommunitynm = './data/case30_QLij_community4.mat'\n",
    "    \n",
    "elif Nbus == 118:\n",
    "    # B118\n",
    "    matnm = './data/XY_case'+str(Nbus)+'r'+str(sys_R)+'real'+'_ipopt_5W'+'.mat'\n",
    "    matparanm = './data/pglib_opf_case'+str(Nbus)+'_ieeer'+str(sys_R)+'_para.mat'\n",
    "    # matcommunitynm = './data/pglib_opf_case118_ieeer4_Z_community6.mat'\n",
    "    matcommunitynm = './data/pglib_opf_case118_ieeer4_SLij_community7.mat'\n",
    "    \n",
    "\n",
    "# matnm = './data/XY_case'+str(Nbus)+'r'+str(sys_R)+'real'+'_ipopt_5W'+'.mat'\n",
    "# matparanm = './data/pglib_opf_case'+str(Nbus)+'_ieeer'+str(sys_R)+'_para.mat' -->\n",
    "# matcommunitynm = './data/pglib_opf_case118_ieeer4_Z_community6.mat'\n",
    "# print('matnm',matnm,'\\nmatparanm',matparanm,'\\nmatcommunitynm',matcommunitynm)\n",
    "\n",
    "mat = scipy.io.loadmat(matnm)   \n",
    "matpara = scipy.io.loadmat(matparanm)\n",
    "# matcommunity = scipy.io.loadmat(matcommunitynm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8960f527-aa3a-47a2-ac6f-0b7f75f02b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus_Va (29,)\n",
      "bus_slack 0\n",
      "Nbus 30 Nbranch 41 maxRPd (50000, 30) maxRQd (50000, 30)\n",
      "gencost [0.02 2.   0.  ]\n",
      "VmLb tensor([0.9800]) VmUb tensor([1.0600]) VaLb -0.29670597283903605 VaUb -0.06981317007977318\n",
      "idx_train (32000,) idx_train_label (3,) idx_test (10000,)\n",
      "bus_Pd (20,) bus_Pd0 (10,) \n",
      "bus_Qd (20,) bus_Qd0 (10,)\n",
      "x (50000, 40)\n",
      "yvmmin 0.95 yvmmax 1.05\n",
      "yvamin -0.10093294776740376 yvamax 0.0\n",
      "his_V (30,)\n",
      "x_tensor torch.Size([50000, 40]) yva_tensor torch.Size([50000, 30]) yvm_tensor torch.Size([50000, 30])\n",
      "Ntrain 32000 xtrain torch.Size([32000, 40]) yvatrain torch.Size([32000, 30]) yvmtrain torch.Size([32000, 30])\n",
      "Ntest 10000 xtest torch.Size([10000, 40]) yvatest torch.Size([10000, 30]) yvmtest torch.Size([10000, 30])\n",
      "yvatrain tensor(-0.1009) tensor(-0.0043)\n",
      "yvmtrain tensor(0.9500) tensor(1.0500)\n",
      "yvatest tensor(-0.1009) tensor(-0.0044)\n",
      "yvmtest tensor(0.9500) tensor(1.0500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# samples and power system parameters\n",
    "if Nbus == 30:\n",
    "    # load 1: with all loads in samples\n",
    "    RPd = mat['RPd']\n",
    "    RQd = mat['RQd']\n",
    "    gen = matpara['gen'][:, [0,3,4,8,9]]\n",
    "    gencost = matpara['gencost'][:, 4:8]\n",
    "    branch = matpara['branch'][:, [0,1,5,11,12]]\n",
    "elif Nbus == 118 or Nbus == 300:\n",
    "    # load 2: with only nonzero loads in samples\n",
    "    load_idx = (mat['load_idx'].squeeze() - 1).astype(int)\n",
    "    print('load_idx',load_idx.shape)\n",
    "    RPd = np.zeros((mat['RPd'].shape[0], Nbus))\n",
    "    RQd = np.zeros((mat['RQd'].shape[0], Nbus))\n",
    "    RPd[:, load_idx] = mat['RPd']\n",
    "    RQd[:, load_idx] = mat['RQd']\n",
    "    \n",
    "    gen = matpara['gen']\n",
    "    gencost = matpara['gencost']\n",
    "    branch = matpara['branch']\n",
    "    \n",
    "RPg = mat['RPg']\n",
    "RQg = mat['RQg']\n",
    "Ybus = matpara['Ybus']\n",
    "Yf = matpara['Yf']\n",
    "Yt = matpara['Yt']\n",
    "bus = matpara['bus']\n",
    "baseMVA = matpara['baseMVA']\n",
    "bus_slack = np.squeeze(np.where(bus[:, 1] == 3))\n",
    "Nbranch = np.size(branch, axis = 0)\n",
    "Ybus_full = Ybus*1.0\n",
    "bus_Va = np.delete(np.arange(Nbus),bus_slack)\n",
    "BRANFT = torch.from_numpy(branch[:, 0:2] - 1).long()\n",
    "print('bus_Va',bus_Va.shape)\n",
    "print('bus_slack',bus_slack)\n",
    "print('Nbus',Nbus,'Nbranch',Nbranch,'maxRPd',mat['RPd'].shape,'maxRQd',mat['RQd'].shape)\n",
    "print('gencost',gencost[0])\n",
    "\n",
    "# incidance matricx of branch from\n",
    "finc = np.zeros((branch.shape[0], Nbus), dtype = float)\n",
    "tinc = np.zeros((branch.shape[0], Nbus), dtype = float)\n",
    "for i in range(branch.shape[0]):\n",
    "    finc[i,  branch[i, 0].astype(int)-1] = 1\n",
    "    tinc[i,  branch[i, 1].astype(int)-1] = 1\n",
    "\n",
    "# narrow down the upper and lower bounds of predicted Va Vm\n",
    "if Nbus == 30:\n",
    "    VmLb = torch.tensor([0.98])\n",
    "    VmUb = torch.tensor([1.06])\n",
    "    VaLb = -math.pi*17/180\n",
    "    VaUb = -math.pi*4/180\n",
    "elif Nbus == 118:\n",
    "    VmLb = torch.tensor([1.02])\n",
    "    VmUb = torch.tensor([1.06])\n",
    "    VaLb = -math.pi*20/180\n",
    "    VaUb = math.pi*16/180\n",
    "print('VmLb', VmLb, 'VmUb', VmUb, 'VaLb', VaLb, 'VaUb', VaUb)\n",
    "\n",
    "# spase matrix of admittance matrix: for computation speedup\n",
    "Ybus = sparse.csr_matrix(Ybus)\n",
    "Yf = sparse.csr_matrix(Yf) \n",
    "Yt = sparse.csr_matrix(Yt)\n",
    "\n",
    "# sample index for random sampling\n",
    "idx_sample = random.sample(range(0,RPd.shape[0]),Nsample)\n",
    "idx_train = np.asarray(idx_sample[0:Ntrain])\n",
    "idx_train_label = np.asarray(idx_sample[Ntrain: Nhis+Ntrain])\n",
    "idx_test = np.asarray(idx_sample[-Ntest:])\n",
    "idx_his = idx_train_label\n",
    "print('idx_train',idx_train.shape,'idx_train_label',idx_train_label.shape,'idx_test',idx_test.shape)\n",
    "      \n",
    "# find bus number of buses with zero and non-zero net power injection: for load satisfaction constraints\n",
    "bus_Pd = np.squeeze(np.where(np.abs(RPd[0,:]) > 0), axis=0)\n",
    "bus_Pd0 = np.squeeze(np.where(RPd[0,:] == 0), axis=0)\n",
    "bus_Qd = np.squeeze(np.where(np.abs(RQd[0,:]) > 0), axis=0)\n",
    "bus_Qd0 = np.squeeze(np.where(RQd[0,:] == 0), axis=0)\n",
    "print('bus_Pd',bus_Pd.shape,'bus_Pd0', bus_Pd0.shape, '\\nbus_Qd',bus_Qd.shape, 'bus_Qd0', bus_Qd0.shape)\n",
    "\n",
    "# input data: non-zero Pd and Qd\n",
    "x = np.concatenate((RPd[:, bus_Pd], RQd[:, bus_Qd]), axis = 1)/baseMVA\n",
    "print('x', x.shape)\n",
    "\n",
    "# output data: Vm Va\n",
    "yvm  = mat['RVm']\n",
    "yva = mat['RVa']*math.pi/180\n",
    "print('yvmmin', np.min(yvm), 'yvmmax', np.max(yvm))\n",
    "print('yvamin', np.min(yva), 'yvamax', np.max(yva))\n",
    "\n",
    "# historical voltage: for post-porocessing\n",
    "his_V = np.mean(yvm[idx_his]*np.exp(1j *yva[idx_his]),axis=0)\n",
    "print('his_V',his_V.shape)\n",
    "\n",
    "# tensor form\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "yva_tensor = torch.from_numpy(yva).float()\n",
    "yvm_tensor = torch.from_numpy(yvm).float()\n",
    "print('x_tensor',x_tensor.shape,'yva_tensor',yva_tensor.shape,'yvm_tensor',yvm_tensor.shape)\n",
    "\n",
    "# since OPF is global, the input of each group has the global information\n",
    "# train data\n",
    "xtrain = x_tensor[idx_train]\n",
    "yvatrain = yva_tensor[idx_train]             \n",
    "yvmtrain = yvm_tensor[idx_train]\n",
    "\n",
    "# test data\n",
    "xtest = x_tensor[idx_test]\n",
    "yvatest = yva_tensor[idx_test]\n",
    "yvmtest = yvm_tensor[idx_test]\n",
    "print('Ntrain',Ntrain,'xtrain', xtrain.shape,'yvatrain', yvatrain.shape,'yvmtrain', yvmtrain.shape)\n",
    "print('Ntest',Ntest,'xtest', xtest.shape,'yvatest', yvatest.shape,'yvmtest', yvmtest.shape)\n",
    "print('yvatrain',torch.min(yvatrain[:, bus_Va]), torch.max(yvatrain[:, bus_Va]))\n",
    "print('yvmtrain',torch.min(yvmtrain), torch.max(yvmtrain))\n",
    "print('yvatest',torch.min(yvatest[:, bus_Va]), torch.max(yvatest[:, bus_Va]))\n",
    "print('yvmtest',torch.min(yvmtest), torch.max(yvmtest))\n",
    "\n",
    "# delete variables that are not used any more\n",
    "del x,x_tensor,yvm,yva,yvm_tensor,yva_tensor\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde1aaa1-3adf-4472-8925-ea44ff5bb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus_Pg (6,) [ 0  1 21 26 22 12] \n",
      "bus_Qg (6,) [ 0  1 21 26 22 12]\n",
      "bus_Pnet_nonPg (18,) [ 2  3  6  7  9 11 13 14 15 16 17 18 19 20 23 25 28 29] \n",
      "bus_Pnet_nonQg (18,) [ 2  3  6  7  9 11 13 14 15 16 17 18 19 20 23 25 28 29]\n",
      "bus_gen [ 0  1 21 26 22 12]\n",
      "bus_Pnet_all (24,) [ 0  1  2  3  6  7  9 11 12 13 14 15 16 17 18 19 20 21 22 23 25 26 28 29] \n",
      "bus_ZIB_all (6,) [ 4  5  8 10 24 27]\n",
      "NZIB 6\n",
      "idx_ZIB (12,)\n",
      "idx_bus_Pnet_slack [0]\n",
      "bus_Pnet_noslack_all [ 1  2  3  6  7  9 11 12 13 14 15 16 17 18 19 20 21 22 23 25 26 28 29]\n",
      "gencost_Pg (6, 3) MAXMIN_Pg (6, 2) MAXMIN_Qg (6, 2)\n"
     ]
    }
   ],
   "source": [
    "# Pg Qg constraints\n",
    "# find the generation that with positive power output: to improve performance\n",
    "# some generation may only output P or Q\n",
    "# find generators really output active/reactive power\n",
    "idxPg = np.squeeze(np.where(gen[:, 3] > 0), axis=0)\n",
    "idxQg = np.squeeze(np.where(gen[:, 1] > 0), axis=0)\n",
    "bus_Pg = gen[idxPg, 0] - 1 # python: bus number start from 0\n",
    "bus_Qg = gen[idxQg, 0] - 1 # python: bus number start from 0\n",
    "bus_Pg = bus_Pg.astype(int)\n",
    "bus_Qg = bus_Qg.astype(int)\n",
    "print('bus_Pg',bus_Pg.shape, bus_Pg,'\\nbus_Qg', bus_Qg.shape,bus_Qg)\n",
    "\n",
    "# numpy to tensor\n",
    "bus_nonPg = np.arange(Nbus)\n",
    "bus_nonPg = torch.from_numpy(np.delete(bus_nonPg,bus_Pg)).long()\n",
    "bus_nonQg = np.arange(Nbus)\n",
    "bus_nonQg = torch.from_numpy(np.delete(bus_nonQg,bus_Qg)).long()\n",
    "\n",
    "# find load buses\n",
    "Pnet_nonPg = RPd[0,:]*1.0\n",
    "Pnet_nonQg = RQd[0,:]*1.0\n",
    "Pnet_nonPg[bus_Pg] = 0\n",
    "Pnet_nonQg[bus_Qg] = 0\n",
    "bus_Pnet_nonPg = np.squeeze(np.where(np.abs(Pnet_nonPg) > 0), axis=0)\n",
    "bus_Pnet_nonQg = np.squeeze(np.where(np.abs(Pnet_nonQg) > 0), axis=0)\n",
    "print('bus_Pnet_nonPg',bus_Pnet_nonPg.shape,bus_Pnet_nonPg,'\\nbus_Pnet_nonQg',bus_Pnet_nonQg.shape,bus_Pnet_nonQg)\n",
    "\n",
    "# bus number of generators\n",
    "bus_gen = gen[:,0].astype(int) - 1\n",
    "print('bus_gen',bus_gen)\n",
    "\n",
    "# find non-ZIB buses\n",
    "Pnet = RPd[0,:]*1.0\n",
    "Pnet[bus_gen] = Pnet[bus_gen] + 10\n",
    "\n",
    "# bus number of non-ZIBs\n",
    "bus_Pnet_all = np.squeeze(np.where(np.abs(Pnet) > 0), axis=0)\n",
    "\n",
    "# bus number of ZIBs\n",
    "bus_ZIB_all = np.squeeze(np.where(np.abs(Pnet) == 0), axis=0)\n",
    "print('bus_Pnet_all',bus_Pnet_all.shape,bus_Pnet_all,'\\nbus_ZIB_all',bus_ZIB_all.shape,bus_ZIB_all)\n",
    "\n",
    "# number of ZIBs\n",
    "NZIB = bus_ZIB_all.shape[0] \n",
    "print('NZIB',NZIB)\n",
    "\n",
    "# index of non-ZIBs\n",
    "idx_Pnet = np.concatenate((bus_Pnet_all, bus_Pnet_all+Nbus), axis=0)\n",
    "\n",
    "# index of ZIBs\n",
    "idx_ZIB = np.concatenate((bus_ZIB_all, bus_ZIB_all+Nbus), axis=0)\n",
    "print('idx_ZIB', idx_ZIB.shape)\n",
    "\n",
    "# index of the slack bus in non-ZIBs\n",
    "idx_bus_Pnet_slack = np.where(bus_Pnet_all == bus_slack)[0]\n",
    "print('idx_bus_Pnet_slack',idx_bus_Pnet_slack)\n",
    "\n",
    "bus_Pnet_noslack_all = np.delete(bus_Pnet_all, idx_bus_Pnet_slack, axis=0)\n",
    "print('bus_Pnet_noslack_all',bus_Pnet_noslack_all)\n",
    "\n",
    "# generation cost coefficients\n",
    "gencost_Pg = gencost[idxPg, :]\n",
    "gencost_tensor = torch.from_numpy(gencost_Pg).float()\n",
    "# get the upper and lower bounds for generators\n",
    "MAXMIN_Pg = gen[idxPg, 3:5]/baseMVA\n",
    "MAXMIN_Qg = gen[idxQg, 1:3]/baseMVA\n",
    "MAXMIN_Pg_tensor = torch.from_numpy(MAXMIN_Pg).float()\n",
    "MAXMIN_Qg_tensor = torch.from_numpy(MAXMIN_Qg).float()\n",
    "print('gencost_Pg',gencost_Pg.shape,'MAXMIN_Pg',MAXMIN_Pg.shape,'MAXMIN_Qg',MAXMIN_Qg.shape)\n",
    "\n",
    "# test load and generation\n",
    "Pdtest = RPd[idx_test]/baseMVA\n",
    "Qdtest = RQd[idx_test]/baseMVA\n",
    "Pgtest = RPg[idx_test][:, idxPg]/baseMVA\n",
    "Qgtest = RQg[idx_test][:, idxQg]/baseMVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742b592c-6f21-469c-a23a-2f6217428a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yvtrain_Pnet torch.Size([32000, 47]) yvttest_Pnet torch.Size([10000, 47])\n",
      "Vmscale torch.Size([24]) tensor(0.0800) Vmbias torch.Size([24]) tensor(0.9800)\n",
      "Vascale torch.Size([23]) tensor(0.2269) Vabias torch.Size([23]) tensor(-0.2967)\n",
      "Vscale torch.Size([47]) tensor(0.2269) Vbias torch.Size([47]) tensor(-0.2967)\n"
     ]
    }
   ],
   "source": [
    "# remove Va of slack bus from the training data\n",
    "yvtrain_Pnet =  torch.cat((yvatrain[:, bus_Pnet_noslack_all.tolist()], yvmtrain[:, bus_Pnet_all.tolist()]), dim=1)\n",
    "yvttest_Pnet =  torch.cat((yvatest[:, bus_Pnet_noslack_all.tolist()], yvmtest[:, bus_Pnet_all.tolist()]), dim=1)\n",
    "print('yvtrain_Pnet',yvtrain_Pnet.shape,'yvttest_Pnet',yvttest_Pnet.shape) \n",
    "\n",
    "# training batch data\n",
    "training_dataset_v = Data.TensorDataset(xtrain, yvtrain_Pnet)\n",
    "training_loader_v = Data.DataLoader(\n",
    "        dataset=training_dataset_v,\n",
    "        batch_size=batch_size_training,\n",
    "        shuffle=True,\n",
    "    )   \n",
    "\n",
    "\n",
    "# test batch data\n",
    "batch_size_test = 1\n",
    "test_dataset_v = Data.TensorDataset(xtest, yvttest_Pnet)\n",
    "test_loader_v = Data.DataLoader(\n",
    "        dataset=test_dataset_v,\n",
    "        batch_size=batch_size_test,\n",
    "        shuffle=False,\n",
    "    ) \n",
    "\n",
    "# number of predicted variables\n",
    "NPred_Vm = bus_Pnet_all.shape[0]\n",
    "NPred_Va = bus_Pnet_noslack_all.shape[0]\n",
    "\n",
    "# scaling and bias of Vm Va\n",
    "Vmscale = torch.ones(NPred_Vm)*(VmUb - VmLb)\n",
    "Vmbias = torch.ones(NPred_Vm)*VmLb\n",
    "print('Vmscale',Vmscale.shape,Vmscale[0],'Vmbias',Vmbias.shape,Vmbias[0])\n",
    "\n",
    "Vascale = torch.ones(NPred_Va)*(VaUb - VaLb)\n",
    "Vabias = torch.ones(NPred_Va)*VaLb\n",
    "print('Vascale',Vascale.shape,Vascale[0],'Vabias',Vabias.shape,Vabias[0])\n",
    "\n",
    "Vscale = torch.cat((Vascale, Vmscale), dim=0)\n",
    "Vbias = torch.cat((Vabias, Vmbias), dim=0)\n",
    "print('Vscale',Vscale.shape,Vscale[0],'Vbias',Vbias.shape,Vbias[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f6eb33-4a0f-455d-9dbe-ce4cb4a30ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea278c0a-9f82-4d5b-ab03-fbc97ea654c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function coefficient\n",
    "Npg, Nqg = bus_Pg.shape[0], bus_Qg.shape[0]\n",
    "Npd, Nqd = bus_Pnet_nonPg.shape[0], bus_Pnet_nonQg.shape[0]\n",
    "if flag_k == 2:\n",
    "    kpd = torch.ones(Npd)\n",
    "    kqd = torch.ones(Nqd)\n",
    "    kgenp = torch.ones(Npg)\n",
    "    kgenq = torch.ones(Nqg)\n",
    "    kv = torch.ones(NZIB)\n",
    "elif flag_k == 1:\n",
    "    kpd = torch.ones(Npd)*kpdi\n",
    "    kqd = torch.ones(Nqd)*kqdi\n",
    "    kgenp = torch.ones(Npg)*kgenpi\n",
    "    kgenq = torch.ones(Nqg)*kgenqi\n",
    "    kv= torch.ones(NZIB)*kvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02b8b186-5eb4-4207-85a9-274504e4920e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_ZIMV (6, 24)\n",
      "param_ZIM (12, 48)\n",
      "Me (60, 60) Mf (60, 60) MGB (30, 60) MBG (30, 60)\n",
      "Me_expd (1, 60, 60) Mf_expd (1, 60, 60)\n",
      "Me_re (50, 60, 60) Mf_re (50, 60, 60)\n",
      "MGB_expd (1, 30, 60) MGB_re (50, 30, 60)\n",
      "MBG_expd (1, 30, 60) MBG_re (50, 30, 60)\n"
     ]
    }
   ],
   "source": [
    "# parameter to recover ZIB: complex form\n",
    "Yyy1 = Ybus_full[bus_ZIB_all, :]\n",
    "Yyy = Yyy1[:, bus_ZIB_all]\n",
    "Yyx1 = Ybus_full[bus_ZIB_all, :]\n",
    "Yyx = Yyx1[:, bus_Pnet_all]\n",
    "param_ZIMV = -np.dot(np.linalg.inv(Yyy),Yyx)\n",
    "print('param_ZIMV',param_ZIMV.shape)\n",
    "\n",
    "# parameters for ZIBs when calculating gradients\n",
    "Yba1 = Ybus_full[bus_ZIB_all]\n",
    "Yba = Yba1[:, bus_Pnet_all]\n",
    "\n",
    "Ybb1 = Ybus_full[bus_ZIB_all]\n",
    "Ybb = Ybb1[:, bus_ZIB_all]\n",
    "\n",
    "Gba, Bba = np.real(Yba), np.imag(Yba)\n",
    "Gbb, Bbb = np.real(Ybb), np.imag(Ybb)\n",
    "Ax_r1 = np.concatenate((Gba, -Bba), axis=1) \n",
    "Ax_r2 = np.concatenate((Bba,  Gba), axis=1) \n",
    "Ax = np.concatenate((Ax_r1,  Ax_r2), axis=0) \n",
    "\n",
    "Ay_r1 = np.concatenate((Gbb, -Bbb), axis=1) \n",
    "Ay_r2 = np.concatenate((Bbb,  Gbb), axis=1) \n",
    "Ay = np.concatenate((Ay_r1,  Ay_r2), axis=0)\n",
    "\n",
    "Ay_inv = np.linalg.inv(Ay)\n",
    "param_ZIM = - np.dot(Ay_inv,Ax)\n",
    "print('param_ZIM',param_ZIM.shape)\n",
    "\n",
    "# Jacobian matrix in Cartesian coordinate system\n",
    "# J[i] = np.dot(diage2, Me) + np.dot(diagf2, Mf) + Mab\n",
    "Me1 = np.concatenate((Ybus_full.real,  -Ybus_full.imag), axis=1)\n",
    "Me2 = np.concatenate((-Ybus_full.imag,  -Ybus_full.real), axis=1)\n",
    "Me = np.concatenate((Me1, Me2), axis=0)\n",
    "\n",
    "Mf1 = np.concatenate((Ybus_full.imag,  Ybus_full.real), axis=1)\n",
    "Mf2 = np.concatenate((Ybus_full.real,  -Ybus_full.imag), axis=1)\n",
    "Mf = np.concatenate((Mf1, Mf2), axis=0)\n",
    "\n",
    "MGB = np.concatenate((Ybus_full.real, -Ybus_full.imag), axis=1)\n",
    "MBG = np.concatenate((Ybus_full.imag, Ybus_full.real), axis=1)\n",
    "print('Me',Me.shape,'Mf',Mf.shape,'MGB',MGB.shape,'MBG',MBG.shape)\n",
    "\n",
    "# for element-wise product (for batch training)\n",
    "Me_expd = np.expand_dims(Me, axis=0)\n",
    "Mf_expd = np.expand_dims(Mf, axis=0)\n",
    "Me_re = np.repeat(Me_expd, batch_size_training, axis=0)\n",
    "Mf_re = np.repeat(Mf_expd, batch_size_training, axis=0)\n",
    "print('Me_expd',Me_expd.shape,'Mf_expd',Mf_expd.shape)\n",
    "print('Me_re',Me_re.shape,'Mf_re',Mf_re.shape)\n",
    "\n",
    "MGB_expd = np.expand_dims(MGB, axis=0)\n",
    "MBG_expd = np.expand_dims(MBG, axis=0)\n",
    "MGB_re = np.repeat(MGB_expd, batch_size_training, axis=0)\n",
    "MBG_re = np.repeat(MBG_expd, batch_size_training, axis=0)\n",
    "print('MGB_expd',MGB_expd.shape,'MGB_re',MGB_re.shape)\n",
    "print('MBG_expd',MBG_expd.shape,'MBG_re',MBG_re.shape)\n",
    "\n",
    "MGB_re_tensor = torch.from_numpy(MGB_re)\n",
    "MBG_re_tensor = torch.from_numpy(MBG_re)\n",
    "Me_re_tensor = torch.from_numpy(Me_re)\n",
    "Mf_re_tensor = torch.from_numpy(Mf_re)\n",
    "param_ZIM_tensor = torch.from_numpy(param_ZIM)\n",
    "param_ZIM_tensor_expd = torch.unsqueeze(param_ZIM_tensor, dim=0)\n",
    "param_ZIM_tensor_re = param_ZIM_tensor_expd.repeat_interleave(batch_size_training, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eedfccf-4a34-4a79-a93c-faca3137f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## other function\n",
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return(rho, phi)\n",
    "\n",
    "def pol2cart(rho, phi):\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    return(x, y)\n",
    "\n",
    "def matrix_diag(x):\n",
    "    # x是二维数组\n",
    "    n, d = x.shape\n",
    "    ret = np.zeros((n, d, d))\n",
    "    index = (np.arange(0, n * d * d, d * d).reshape(-1, 1) +\n",
    "          np.arange(0, d * d, d + 1)).reshape(-1)\n",
    "    ret.flat[index] = x.reshape(-1)\n",
    "    return ret\n",
    "\n",
    "def tensor_coo2d(x):\n",
    "    idx = torch.nonzero(x).T\n",
    "    data = x[idx[0],idx[1],idx[2]]\n",
    "    coo_x = torch.sparse_coo_tensor(idx, data, x.shape)\n",
    "    return coo_x\n",
    "\n",
    "def get_clamp(Pred, Predmin, Predmax):\n",
    "    # each row is a sample;Predmin and Predmax is the limit for each element of each row\n",
    "    Pred_clip = Pred.clamp(min = Predmin, max = Predmax)\n",
    "    return Pred_clip\n",
    "\n",
    "# testing Vm\n",
    "def get_mae(real, predict):\n",
    "    '''\n",
    "    mean absolute error\n",
    "    '''\n",
    "    if len(real) == len(predict):\n",
    "        if torch.is_tensor(real):\n",
    "            err = torch.mean(torch.abs(real - predict)) \n",
    "        else:\n",
    "            err = np.mean(np.abs(real - predict))\n",
    "            \n",
    "        return err\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_rerr(real, predict):\n",
    "    '''\n",
    "    relative error\n",
    "    '''\n",
    "    if len(real) == len(predict):\n",
    "        if torch.is_tensor(real):\n",
    "            err = torch.abs((predict - real) / real)*100\n",
    "        else:\n",
    "            err = np.abs((predict - real) / real)*100\n",
    "        return err\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_rerr2(real, predict):\n",
    "    '''\n",
    "    relative error\n",
    "    '''\n",
    "    if len(real) == len(predict):       \n",
    "        err = (predict - real) / real*100          \n",
    "        return err\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# power balance\n",
    "def get_PQ(V):\n",
    "    S = np.zeros(V.shape,dtype = 'complex_')\n",
    "    for i in range(V.shape[0]):\n",
    "#         I = np.dot(Ybus, V[i]).conj()\n",
    "        I = Ybus.dot(V[i]).conj()\n",
    "        S[i]  = np.multiply(V[i], I)\n",
    "    \n",
    "    P = np.real(S)\n",
    "    Q = np.imag(S) \n",
    "    return P, Q\n",
    "        \n",
    "def get_genload(V, Pdtest, Qdtest, bus_Pg, bus_Qg):\n",
    "    S = np.zeros(V.shape,dtype = 'complex_')\n",
    "    for i in range(V.shape[0]):\n",
    "#         I = np.dot(Ybus, V[i]).conj()\n",
    "        I = Ybus.dot(V[i]).conj()\n",
    "        S[i]  = np.multiply(V[i], I)\n",
    "    \n",
    "    P = np.real(S)\n",
    "    Q = np.imag(S) \n",
    "\n",
    "    # bus_Pg = bus_Pg.cpu()\n",
    "    # bus_Qg = bus_Qg.cpu()\n",
    "    Pg = P[:, bus_Pg] + Pdtest[:, bus_Pg]\n",
    "    Qg = Q[:, bus_Qg] + Qdtest[:, bus_Qg]   \n",
    "    Pd = -P\n",
    "    Qd = -Q\n",
    "    Pd[:, bus_Pg] = Pg - P[:, bus_Pg]\n",
    "    Qd[:, bus_Qg] = Qg - Q[:, bus_Qg]   \n",
    "    return Pg, Qg, Pd, Qd\n",
    "\n",
    "def get_Pgcost(Pg,idxPg,gencost):\n",
    "    cost = (gencost[:, 0]*(Pg*Pg) + gencost[:, 1]*Pg)   # cost = a*Pg^2+b*Pg+c\n",
    "    return np.sum(cost, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c0da6fc-b0d7-417c-bf4e-db9caffcc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.pow(input,exponent)\n",
    "class Penalty_V(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, V, PQd):\n",
    "        Nsam = V.shape[0]  \n",
    "        xam_P = np.insert(V.detach().cpu().numpy(), idx_bus_Pnet_slack, 0, axis=1) # insert slack bus fi = 0\n",
    "        Vx = xam_P[:, NPred_Vm:]*np.exp(1j*xam_P[:, 0:NPred_Vm]) \n",
    "        # print('Vm', xam_P[7, NPred_Vm:], '\\nVa', xam_P[7, 0:NPred_Vm])\n",
    "        \n",
    "        # recover ZIBs Va Vm\n",
    "        Vy = np.dot(param_ZIMV, Vx.T).T\n",
    "        # print('Vx',Vx.shape,'Vy',Vy.shape,)\n",
    "        \n",
    "        Ve = np.zeros((Nsam, Nbus));\n",
    "        Vf = np.zeros((Nsam, Nbus));\n",
    "        Ve[:, bus_Pnet_all] = Vx.real\n",
    "        Vf[:, bus_Pnet_all] = Vx.imag\n",
    "        Ve[:, bus_ZIB_all] = Vy.real\n",
    "        Vf[:, bus_ZIB_all] = Vy.imag   \n",
    "        Pred_V = Ve + 1j*Vf\n",
    "        \n",
    "        PQd = PQd.detach()\n",
    "        Pdtest = torch.zeros((Nsam, Nbus)).float().to(device) # all buses\n",
    "        Qdtest = torch.zeros((Nsam, Nbus)).float().to(device) # all buses\n",
    "        Pdtest[:, bus_Pd] = PQd[:, 0: bus_Pd.shape[0]]# none zero load (may contain generation bus)\n",
    "        Qdtest[:, bus_Qd] = PQd[:, bus_Pd.shape[0]:] # none zero load (may contain generation bus)\n",
    "\n",
    "        # S = P + j*Q; P = Pg - Pd; Q = Qg - Qd;\n",
    "        Pred_S = np.zeros(Ve.shape, dtype = 'complex_') \n",
    "        Pred_I = np.zeros(Ve.shape, dtype = 'complex_') \n",
    "        for i in range(Nsam):\n",
    "            Pred_I[i] = Ybus.dot(Pred_V[i]).conj()\n",
    "            Pred_S[i] = np.multiply(Pred_V[i], Pred_I[i])\n",
    "\n",
    "        Pred_P = torch.from_numpy(np.real(Pred_S)).float().to(device)\n",
    "        Pred_Q = torch.from_numpy(np.imag(Pred_S)).float().to(device)\n",
    "        Pg = Pred_P + Pdtest\n",
    "        Qg = Pred_Q + Qdtest\n",
    "        \n",
    "        # print('Pg[:, bus_Pg]', Pg[0, bus_Pg],'\\nQg[:, bus_Pg]', Qg[0, bus_Qg])\n",
    "\n",
    "        # generator limits  \n",
    "        loss_Pgi = torch.sum((torch.clamp(Pg[:, bus_Pg] - MAXMIN_Pg_tensor[:, 1], max=0).pow(2) +  \\\n",
    "                   torch.clamp(Pg[:, bus_Pg] - MAXMIN_Pg_tensor[:, 0], min=0).pow(2)), dim = 0)\n",
    "        \n",
    "        loss_Qgi = torch.sum((torch.clamp(Qg[:, bus_Qg] - MAXMIN_Qg_tensor[:, 1], max=0).pow(2) + \\\n",
    "                   torch.clamp(Qg[:, bus_Qg] - MAXMIN_Qg_tensor[:, 0], min=0).pow(2)), dim = 0)\n",
    "        \n",
    "        mat_PgLB = torch.clamp(Pg[:, bus_Pg] - MAXMIN_Pg_tensor[:, 1], max=0)\n",
    "        mat_PgUB = torch.clamp(Pg[:, bus_Pg] - MAXMIN_Pg_tensor[:, 0], min=0)\n",
    "        mat_QgLB = torch.clamp(Qg[:, bus_Qg] - MAXMIN_Qg_tensor[:, 1], max=0)\n",
    "        mat_QgUB = torch.clamp(Qg[:, bus_Qg] - MAXMIN_Qg_tensor[:, 0], min=0)\n",
    "        \n",
    "        # print('mat_PgLB', mat_PgLB[0],'\\nmat_PgUB', mat_PgUB[0],'\\nmat_QgLB', mat_QgLB[0],'\\nmat_QgUB', mat_QgUB[0])\n",
    "        # print('loss_Pgi', loss_Pgi,'\\nloss_Qgi', loss_Qgi)\n",
    "\n",
    "        loss_Pg = loss_Pgi.sum()   \n",
    "        loss_Qg = loss_Qgi.sum()\n",
    "            \n",
    "        # for non generation bus\n",
    "        loss_Pdi = torch.sum(Pg[:, bus_Pnet_nonPg].pow(2), dim = 0)\n",
    "        loss_Qdi = torch.sum(Qg[:, bus_Pnet_nonQg].pow(2), dim = 0)\n",
    "        \n",
    "        # loss_Pdi = torch.sum(Pg[:, bus_nonPg].pow(2), dim = 0)\n",
    "        # loss_Qdi = torch.sum(Qg[:, bus_nonQg].pow(2), dim = 0)\n",
    "        # print('loss_Pdi', loss_Pdi[0], 'loss_Qdi', loss_Qdi[0])\n",
    "        # print('loss_ZIB_Pd', Pg[0, bus_ZIB_all], 'loss_ZIB_Qd', Qg[0, bus_ZIB_all])\n",
    "\n",
    "        loss_Pd = loss_Pdi.sum()\n",
    "        loss_Qd = loss_Qdi.sum()\n",
    "\n",
    "        # Pg cost; ignore constant c0        \n",
    "        absPg = torch.where(Pg[:, bus_Pg] > 0, Pg[:, bus_Pg], -Pg[:, bus_Pg]*2.0)\n",
    "        # print('absPg',absPg.shape,'gencost_tensor',gencost_tensor.shape)\n",
    "        loss_Pgcost = (gencost_tensor[:, 0]*torch.pow(Pg[:, bus_Pg], 2) + gencost_tensor[:, 1]*absPg).sum()\n",
    "        \n",
    "        # volatge violation of ZIB\n",
    "        Vm_ZIB = torch.from_numpy(np.sqrt(Ve[:, bus_ZIB_all]**2 + Vf[:, bus_ZIB_all]**2)).to(device)\n",
    "        loss_Vi = torch.sum((torch.clamp(Vm_ZIB - VmLb[0], max=0).pow(2) +  \\\n",
    "                             torch.clamp(Vm_ZIB - VmUb[0], min=0).pow(2)), dim = 0)\n",
    "        \n",
    "        # total loss\n",
    "        global kcost, kpd, kqd, kgenp, kgenq, kv\n",
    "        if flag_k == 2:\n",
    "            # update penalty coefficient \n",
    "            kgenp = torch.min(kcost*loss_Pgcost/(loss_Pgi+1e-4), kgenp_max)\n",
    "            kgenq = torch.min(kcost*loss_Pgcost/(loss_Qgi+1e-4), kgenq_max)\n",
    "            kpd = torch.min(kcost*loss_Pgcost/(loss_Pdi+1e-4), kpd_max)\n",
    "            kqd = torch.min(kcost*loss_Pgcost/(loss_Qdi+1e-4), kqd_max)\n",
    "            kv = torch.min(kcost*loss_Pgcost/(loss_Vi+1e-4), kv_max)\n",
    "        \n",
    "        ls_cost  = (kcost*loss_Pgcost)/Nsam\n",
    "        ls_Pg = (kgenp*loss_Pgi).sum()/Nsam\n",
    "        ls_Qg = (kgenq*loss_Qgi).sum()/Nsam\n",
    "        ls_Pd = (kpd*loss_Pdi).sum()/Nsam\n",
    "        ls_Qd = (kqd*loss_Qdi).sum()/Nsam \n",
    "        ls_V = (kv*loss_Vi).sum()/Nsam\n",
    "    \n",
    "        loss_out = ls_cost + ls_Pg + ls_Qg + ls_Pd + ls_Qd + ls_V\n",
    "        # print('ls_cost',ls_cost,'ls_Pg',ls_Pg,'ls_Qg',ls_Qg,'ls_Pd',ls_Pd,'ls_Qd',ls_Qd,'ls_V',ls_V)\n",
    "        \n",
    "        if flag_save_ls:           \n",
    "            ls_costk_log.append(kcost*loss_Pgcost)\n",
    "            ls_Pgk_log.append(kgenp*loss_Pgi)\n",
    "            ls_Qgk_log.append(kgenq*loss_Qgi)\n",
    "            ls_Pdk_log.append(kpd*loss_Pdi)\n",
    "            ls_Qdk_log.append(kqd*loss_Qdi)\n",
    "            ls_Vk_log.append(kv*loss_Vi)\n",
    "\n",
    "            ls_cost_log.append(loss_Pgcost)\n",
    "            ls_Pg_log.append(loss_Pgi)\n",
    "            ls_Qg_log.append(loss_Qgi)\n",
    "            ls_Pd_log.append(loss_Pdi)\n",
    "            ls_Qd_log.append(loss_Qdi)\n",
    "            ls_V_log.append(loss_Vi)\n",
    "\n",
    "            k_Pg_log.append(kgenp)\n",
    "            k_Qg_log.append(kgenq)\n",
    "            k_Pd_log.append(kpd)\n",
    "            k_Qd_log.append(kqd) \n",
    "            k_V_log.append(kv)\n",
    "        \n",
    "        # save variables used in backward: calculate gradient\n",
    "        ctx.save_for_backward(Pg, Qg, torch.as_tensor(Ve), torch.as_tensor(Vf),torch.as_tensor(kgenp), torch.as_tensor(kgenq), \\\n",
    "                              torch.as_tensor(kpd), torch.as_tensor(kqd), torch.as_tensor(xam_P)) \n",
    "        return loss_out\n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        Pg, Qg, Ve, Vf, kgenp, kgenq, kpd, kqd, xam_P = ctx.saved_tensors\n",
    "        \n",
    "        Ve_tensor = Ve.to(device)\n",
    "        Vf_tensor = Vf.to(device)\n",
    "        Vef_tensor = torch.cat((Ve_tensor, Vf_tensor), dim=1)\n",
    "        xam_P_tensor = xam_P.to(device)\n",
    "        \n",
    "        Pg, Qg, Ve, Vf = Pg.detach(), Qg.detach(), Ve.detach(), Vf.detach()\n",
    "        kgenp, kgenq, kpd, kqd = kgenp.detach(), kgenq.detach(), kpd.detach(), kqd.detach()  \n",
    "        Nsam, Nbus = Ve.shape\n",
    "        \n",
    "        #**************************************************************************#\n",
    "        # dLv/dVm\n",
    "        Vm_ZIB = torch.sqrt(Ve[:, bus_ZIB_all]**2 + Vf[:, bus_ZIB_all]**2).to(device)\n",
    "        # Voltage violation: d(Vm_hat - VmUb)dVef;d(Vm_hat - VmLb)dVef\n",
    "        mat_Vmax = torch.where(Vm_ZIB - VmUb[0] >  kdelta, ((Vm_ZIB - VmUb[0])/Vm_ZIB), torch.tensor([0.0]).double().to(device))\n",
    "        mat_Vmin = torch.where(Vm_ZIB - VmLb[0] < -kdelta, ((Vm_ZIB - VmLb[0])/Vm_ZIB), torch.tensor([0.0]).double().to(device))\n",
    "        mat_V = (mat_Vmin + mat_Vmax)*kv \n",
    "        mat_V2 = torch.cat((mat_V, mat_V),dim = 1)   \n",
    "        # print('Vm_ZIB', Vm_ZIB.shape,'mat_Vmin', mat_Vmin.shape,'mat_Vmax', mat_Vmax.shape)\n",
    "        # print('mat_V2', mat_V2.shape)\n",
    "        # print('Vm_ZIB',Vm_ZIB[0],'\\nmat_Vmin',mat_Vmin[0],'\\nmat_Vmax',mat_Vmax[0])\n",
    "        \n",
    "        #**************************************************************************#\n",
    "        # dLPgQgPdQd/dPQ\n",
    "        mat_P = torch.zeros(Nsam, Nbus).to(device)\n",
    "        mat_Q = torch.zeros(Nsam, Nbus).to(device) \n",
    "        \n",
    "        # symbal of Pg: Relu(Pg_hat - Pg_max) + Relu(Pg_min - Pg_hat)\n",
    "        # loss_Pg.sum()\n",
    "        #元素小于0，取1；否则取0\n",
    "        mat_Pgmin = torch.where(Pg[:, bus_Pg] - MAXMIN_Pg_tensor[:, 1] < -kdelta, 2*(Pg[:, bus_Pg] - MAXMIN_Pg_tensor[:, 1]).to(device), torch.tensor([0.0]).to(device))\n",
    "        #元素大于0，取1；否则取0\n",
    "        mat_Pgmax = torch.where(Pg[:, bus_Pg] - MAXMIN_Pg_tensor[:, 0] > kdelta, 2*(Pg[:, bus_Pg] - MAXMIN_Pg_tensor[:, 0]).to(device), torch.tensor([0.0]).to(device)) \n",
    "        # print('mat_Pgmin', mat_Pgmin.shape, mat_Pgmin[0], 'mat_Pgmax', mat_Pgmax.shape, mat_Pgmax[0],)\n",
    "\n",
    "        # Pg cost\n",
    "        mat_Pgneg = torch.where(Pg[:, bus_Pg] > 0, torch.tensor([1.0]).to(device), torch.tensor([-2.0]).to(device))  \n",
    "        mat_Pgcost = ((2*gencost_tensor[:, 0]).repeat(Nsam, 1)*Pg[:, bus_Pg] + gencost_tensor[:, 1].repeat(Nsam, 1)*mat_Pgneg)  \n",
    "        # print('mat_Pgneg', mat_Pgcost.shape, mat_Pgneg[0])\n",
    "            \n",
    "        mat_P[:, bus_Pg] = (mat_Pgmin+mat_Pgmax)*kgenp.reshape(1, -1) + mat_Pgcost*kcost\n",
    "     \n",
    "        # loss_Qg.sum()\n",
    "        #元素小于0，取1；否则取0\n",
    "        mat_Qgmin = torch.where(Qg[:, bus_Qg] - MAXMIN_Qg_tensor[:, 1] < -kdelta, 2*(Qg[:, bus_Qg] - MAXMIN_Qg_tensor[:, 1]).to(device), torch.tensor([0.0]).to(device)) \n",
    "        #元素大于0，取1；否则取0\n",
    "        mat_Qgmax = torch.where(Qg[:, bus_Qg] - MAXMIN_Qg_tensor[:, 0] > kdelta, 2*(Qg[:, bus_Qg] - MAXMIN_Qg_tensor[:, 0]).to(device), torch.tensor([0.0]).to(device))       \n",
    "        mat_Q[:, bus_Qg] = (mat_Qgmin + mat_Qgmax)*kgenq.reshape(1, -1)\n",
    "        # print('mat_Qgmin', mat_Qgmin.shape, 'mat_Qgmax', mat_Qgmax.shape)\n",
    "        # print('kgenq.reshape(1, -1)', kgenq.reshape(1, -1))\n",
    "\n",
    "        # loss_PQd\n",
    "        mat_P[:, bus_Pnet_nonPg] = (torch.where(torch.abs(Pg[:, bus_Pnet_nonPg]) > kdelta, 2*Pg[:, bus_Pnet_nonPg].to(device), torch.tensor([0.0]).to(device)))*kpd.reshape(1, -1) # matrix with 1 or -1\n",
    "        mat_Q[:, bus_Pnet_nonQg] = (torch.where(torch.abs(Qg[:, bus_Pnet_nonQg]) > kdelta, 2*Qg[:, bus_Pnet_nonQg].to(device), torch.tensor([0.0]).to(device)))*kqd.reshape(1, -1) # matrix with 1 or -1\n",
    "        # mat_P[:, bus_nonPg] = (torch.where(torch.abs(Pg[:, bus_nonPg]) > kdelta, 2*Pg[:, bus_nonPg].to(device), torch.tensor([0.0]).to(device)))*kpd.reshape(1, -1) # matrix with 1 or -1\n",
    "        # mat_Q[:, bus_nonQg] = (torch.where(torch.abs(Qg[:, bus_nonQg]) > kdelta, 2*Qg[:, bus_nonQg].to(device), torch.tensor([0.0]).to(device)))*kqd.reshape(1, -1) # matrix with 1 or -1\n",
    "        # print('mat_P[:, bus_nonPg]', mat_P[:, bus_nonPg].shape, 'mat_Q[:, bus_nonQg]', mat_Q[:, bus_nonQg].shape, \"kpd.reshape(1, -1)\", (kpd.reshape(1, -1)).shape)\n",
    "        \n",
    "        # dPdVm\n",
    "        mat_P3 = torch.unsqueeze(mat_P[:, bus_Pnet_all], 2)\n",
    "        mat_Q3 = torch.unsqueeze(mat_Q[:, bus_Pnet_all], 2)\n",
    "        # print('mat_P3',mat_P3.shape,'mat_Q3',mat_Q3.shape)\n",
    "  \n",
    "        mat_PQ3 = torch.cat((mat_P3, mat_Q3), dim=1)\n",
    "        # print('mat_PQ3', mat_PQ3.shape)\n",
    " \n",
    "        #*****************************Jacobian matrix***********************************# \n",
    "        # start_time = time.process_time()\n",
    "        diage_expd_tensor = torch.unsqueeze(torch.cat((Ve_tensor, Ve_tensor), dim=1), dim=2)\n",
    "        diagf_expd_tensor = torch.unsqueeze(torch.cat((Vf_tensor, Vf_tensor), dim=1), dim=2)\n",
    "        # print('diage_expd_tensor',diage_expd_tensor.shape,'diagf_expd_tensor',diagf_expd_tensor.shape)\n",
    "        # print('Ve_tensor',Ve_tensor.device,'diagf_expd_tensor',diagf_expd_tensor.device)\n",
    "        \n",
    "        diage_re_tensor = diage_expd_tensor.repeat_interleave(2*Nbus, dim=2)\n",
    "        diagf_re_tensor = diagf_expd_tensor.repeat_interleave(2*Nbus, dim=2)\n",
    " \n",
    "        Vef_expd_tensor = torch.unsqueeze(Vef_tensor, dim=1)\n",
    "        Vef_re_tensor = Vef_expd_tensor.repeat_interleave(Nbus, dim=1)\n",
    "        # print('Vef_re_tensor',Vef_re_tensor.device)\n",
    "        \n",
    "        a_tensor = torch.sum(MGB_re_tensor.to(device)*Vef_re_tensor, dim=2)\n",
    "        b_tensor = torch.sum(MBG_re_tensor.to(device)*Vef_re_tensor, dim=2)\n",
    "        a_diag_tensor = torch.as_tensor(matrix_diag(a_tensor.cpu().numpy()))\n",
    "        b_diag_tensor = torch.as_tensor(matrix_diag(b_tensor.cpu().numpy()))\n",
    "        \n",
    "        Mab_diag1_tensor = torch.cat((a_diag_tensor.to(device), b_diag_tensor.to(device)), dim=2)\n",
    "        Mab_diag2_tensor = torch.cat((-b_diag_tensor.to(device), a_diag_tensor.to(device)), dim=2)\n",
    "        Mab_diag_tensor = torch.cat((Mab_diag1_tensor, Mab_diag2_tensor), dim=1)\n",
    "\n",
    "        # Jacobian matrix\n",
    "        J_tensor = diage_re_tensor*Me_re_tensor.to(device) + diagf_re_tensor*Mf_re_tensor.to(device) + Mab_diag_tensor\n",
    "        # Jacobian matrix for non-ZIBs: dPQ_Pnet/dVef_Pnet\n",
    "        Jx1_tensor = J_tensor[:, idx_Pnet, :]\n",
    "        Jx_tensor = Jx1_tensor[:, :, idx_Pnet]\n",
    "        # print('Jx', Jx.shape)\n",
    "        \n",
    "        # Jacobian matrix for ZIBs: dPQ_Pnet/dVef_ZIB\n",
    "        Jy1_tensor = J_tensor[:, idx_Pnet, :]\n",
    "        Jy_tensor = Jy1_tensor[:, :, idx_ZIB]\n",
    "        # print('Jy', Jy.shape,'param_ZIM_tensor_re', param_ZIM_tensor_re.shape)\n",
    "\n",
    "        # dPQ_Pnet/dVef_ZIB*dVef_ZIB/Vef_Pnet\n",
    "        Jyx_tensor = torch.bmm(Jy_tensor, param_ZIM_tensor_re.to(device))\n",
    "        \n",
    "        # dPQ_Pnet/dVef_Pnet: non-ZIBs and ZIBs\n",
    "        Jcom_tensor = Jx_tensor + Jyx_tensor\n",
    "        # print('Jcom', Jcom.shape)\n",
    "        \n",
    "        #**************************************************************************#\n",
    "        # dVef_Pnet/dVam_Pnet: from Cartesian coordinate system to polar coordinates\n",
    "        Vax_tensor, Vmx_tensor = xam_P_tensor[:, 0:NPred_Vm], xam_P_tensor[:, NPred_Vm:]\n",
    "        # print('Vax', Vax.shape,'Vmx', Vmx.shape)\n",
    "\n",
    "        # dPQ/dVef\n",
    "        dPQdVe_tensor = Jcom_tensor[:, :, 0: NPred_Vm];\n",
    "        dPQdVf_tensor = Jcom_tensor[:, :, NPred_Vm:];\n",
    "        # print('dPQdVe', dPQdVe.shape,'dPQdVf', dPQdVf.shape)\n",
    "        \n",
    "        dPQdVe2_tensor = torch.cat((dPQdVe_tensor, dPQdVe_tensor), dim = 2)\n",
    "        dPQdVf2_tensor = torch.cat((dPQdVf_tensor, dPQdVf_tensor), dim = 2)\n",
    "        # print('dPQdVf2', dPQdVf2.shape,'dPQdVe2', dPQdVe2.shape)\n",
    " \n",
    "        # dVef/dVam\n",
    "        dVedVa_tensor = -Vmx_tensor*torch.sin(Vax_tensor) \n",
    "        dVedVm_tensor = torch.cos(Vax_tensor)\n",
    "        # print('dVedVm', dVedVm.shape,'dVedVa', dVedVa.shape)\n",
    "        dVfdVa_tensor = Vmx_tensor*torch.cos(Vax_tensor)\n",
    "        dVfdVm_tensor = torch.sin(Vax_tensor)\n",
    "        \n",
    "        dVedVa_expd_tensor = torch.unsqueeze(dVedVa_tensor, dim=1)\n",
    "        dVedVm_expd_tensor = torch.unsqueeze(dVedVm_tensor, dim=1)\n",
    "        # print('dVedVm_expd', dVedVm_expd.shape,'dVedVa_expd', dVedVa_expd.shape)\n",
    "        dVfdVa_expd_tensor = torch.unsqueeze(dVfdVa_tensor, dim=1)\n",
    "        dVfdVm_expd_tensor = torch.unsqueeze(dVfdVm_tensor, dim=1)\n",
    "\n",
    "        dVedVa_rep_tensor = dVedVa_expd_tensor.repeat_interleave(NPred_Vm, dim=1)\n",
    "        dVedVm_rep_tensor = dVedVm_expd_tensor.repeat_interleave(NPred_Vm, dim=1)\n",
    "        # print('dVedVm_rep', dVedVm_rep.shape,'dVedVa_rep', dVedVa_rep.shape)\n",
    "        dVfdVa_rep_tensor = dVfdVa_expd_tensor.repeat_interleave(NPred_Vm, dim=1)\n",
    "        dVfdVm_rep_tensor = dVfdVm_expd_tensor.repeat_interleave(NPred_Vm, dim=1)\n",
    "   \n",
    "        dVedVam_tensor = torch.cat((dVedVa_rep_tensor, dVedVm_rep_tensor), dim = 2)\n",
    "        dVfdVam_tensor = torch.cat((dVfdVa_rep_tensor, dVfdVm_rep_tensor), dim = 2)\n",
    "        dVe2dVam_tensor = torch.cat((dVedVam_tensor, dVedVam_tensor), dim = 1)\n",
    "        dVf2dVam_tensor = torch.cat((dVfdVam_tensor, dVfdVam_tensor), dim = 1)\n",
    "        # print('dVedVam', dVedVam.shape,'dVe2dVam', dVe2dVam.shape)\n",
    "        \n",
    "        # dPQd/dVef*dVefx/dVamx: for Pg Qg Pd Qd constaint violation\n",
    "        Jcom_pole_tensor = dPQdVe2_tensor*dVe2dVam_tensor + dPQdVf2_tensor*dVf2dVam_tensor\n",
    "        J_slack_tensor = torch.cat((Jcom_pole_tensor[:, :, :idx_bus_Pnet_slack[0]], Jcom_pole_tensor[:, :, idx_bus_Pnet_slack[0]+1:]), dim = 2)\n",
    "        #**************************************************************************#\n",
    "        # dLv/dVam: for voltage violaton\n",
    "        Vefy_tensor = Vef_tensor[:, idx_ZIB]\n",
    "        dLdVefy_tensor = Vefy_tensor*mat_V2\n",
    "        # print('dLdVefy', dLdVefy.shape)\n",
    "        dLdVefy_expd_tensor = torch.unsqueeze(dLdVefy_tensor, dim=2)\n",
    "        dLdVefy_re_tensor = dLdVefy_expd_tensor.repeat_interleave(2*NPred_Vm, dim=2)\n",
    "        dLdVefx_tensor = dLdVefy_re_tensor*param_ZIM_tensor_re.to(device)\n",
    "        # print('dLdVefx_tensor', dLdVefx_tensor.shape,'dLdVefy_re_tensor', dLdVefy_re_tensor.shape,'param_ZIM_tensor_re', param_ZIM_tensor_re.shape)\n",
    "        dLdVex2_tensor = torch.cat((dLdVefx_tensor[:, :, 0:NPred_Vm], dLdVefx_tensor[:, :, 0:NPred_Vm]), dim = 2)\n",
    "        dLdVfx2_tensor = torch.cat((dLdVefx_tensor[:, :, NPred_Vm:], dLdVefx_tensor[:, :, NPred_Vm:]), dim = 2)\n",
    "        # print('dLdVex2', dLdVex2.shape,'dLdVfx2', dLdVfx2.shape)\n",
    "        dLdVefx2_tensor = torch.cat((dLdVex2_tensor, dLdVfx2_tensor), dim = 1)\n",
    "        # print('dLdVefx2', dLdVefx2.shape)\n",
    "\n",
    "        dVedVam_expd_tensor = torch.cat((dVedVa_expd_tensor, dVedVm_expd_tensor), dim = 2)\n",
    "        dVfdVam_expd_tensor = torch.cat((dVfdVa_expd_tensor, dVfdVm_expd_tensor), dim = 2)\n",
    "        dVedVamy_re_tensor = dVedVam_expd_tensor.repeat_interleave(2*NZIB, dim=1)\n",
    "        dVfdVamy_re_tensor = dVfdVam_expd_tensor.repeat_interleave(2*NZIB, dim=1)\n",
    "        dVefdVamy_tensor = torch.cat((dVedVamy_re_tensor, dVfdVamy_re_tensor), dim = 1)\n",
    "        # print('dVedVam_expd', dVedVam_expd.shape,'dVfdVam_expd', dVfdVam_expd.shape)\n",
    "        # print('dVedVamy_re', dVedVamy_re.shape,'dVfdVamy_re', dVfdVamy_re.shape)\n",
    "        # print('dVefdVamy', dVefdVamy.shape)\n",
    "        \n",
    "        dLdVamx_tensor = dLdVefx2_tensor*dVefdVamy_tensor\n",
    "        dLdVamx_slack_tensor = torch.cat((dLdVamx_tensor[:, :, :idx_bus_Pnet_slack[0]], dLdVamx_tensor[:, :, idx_bus_Pnet_slack[0]+1:]), dim = 2)\n",
    "        # print('dLdVamx', dLdVamx.shape,'dLdVamx_slack', dLdVamx_slack.shape)\n",
    "  \n",
    "        matJ_tensor = mat_PQ3*J_slack_tensor\n",
    "        # grad_Vloss_unsort = torch.sum(matJ_tensor, dim = 1) + torch.sum(dLdVamx_slack_tensor, dim=1)\n",
    "        # grad_Vloss = grad_Vloss_unsort[:, idx_bus_Pnet_sort.tolist()]\n",
    "        \n",
    "        grad_Vloss = torch.sum(matJ_tensor, dim = 1) + torch.sum(dLdVamx_slack_tensor, dim=1)\n",
    "        \n",
    "        return grad_output.to(device)*grad_Vloss.to(device), None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d393a7e6-ecd3-4b9d-b4ac-da62f539824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetV(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, hidden_units,khidden,Vscale,Vbias):\n",
    "        super(NetV, self).__init__()\n",
    "        self.num_layer = khidden.shape[0]\n",
    "        self.fc1 = nn.Linear(input_channels, khidden[0]*hidden_units)\n",
    "        self.scale = Vscale.to(device)\n",
    "        self.bias = Vbias.to(device)\n",
    "        \n",
    "        if self.num_layer >= 2: \n",
    "            self.fc2 = nn.Linear(khidden[0]*hidden_units, khidden[1]*hidden_units)\n",
    "        \n",
    "        if self.num_layer >= 3:\n",
    "            self.fc3 = nn.Linear(khidden[1]*hidden_units, khidden[2]*hidden_units)\n",
    "            \n",
    "            \n",
    "        self.fcbfend = nn.Linear(khidden[khidden.shape[0]-1]*hidden_units, output_channels)\n",
    "        self.fcend = nn.Linear(output_channels, output_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))    \n",
    "        if self.num_layer >= 2:\n",
    "            x = F.relu(self.fc2(x))\n",
    "            \n",
    "        if self.num_layer >= 3:\n",
    "            x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # fixed final two layers\n",
    "        x = F.relu(self.fcbfend(x))\n",
    "        \n",
    "        x = self.fcend(x)  \n",
    "        x_PredV = (torch.sigmoid(x)*self.scale + self.bias)\n",
    "    \n",
    "        return x_PredV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fa28a8a-851f-49ef-b3d8-0bb3446628ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural setting\n",
    "input_channels = xtrain.shape[1]\n",
    "MSE_loss = nn.MSELoss()\n",
    "func_loss = Penalty_V()\n",
    "output_channels = NPred_Va + NPred_Vm\n",
    "# train model if it is not test\n",
    "if flag_test == 0:\n",
    "    time_trian = 0\n",
    "    loss_log = []\n",
    "    # initialize model\n",
    "    model = NetV(input_channels, output_channels, hidden_units, khidden, Vscale, Vbias)\n",
    "    # initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Lr)\n",
    "\n",
    "    # load model to GPU \n",
    "    if torch.cuda.is_available():\n",
    "        model.to(device)\n",
    "        print('model.to(device)')\n",
    "        \n",
    "        MAXMIN_Pg_tensor = MAXMIN_Pg_tensor.to(device)\n",
    "        MAXMIN_Qg_tensor = MAXMIN_Qg_tensor.to(device)\n",
    "        \n",
    "        # rev\n",
    "        bus_Pg = torch.tensor(bus_Pg).to(device)        \n",
    "        bus_Qg = torch.tensor(bus_Qg).to(device)\n",
    "        bus_Pd = torch.tensor(bus_Pd).to(device)\n",
    "        bus_Qd = torch.tensor(bus_Qd).to(device)\n",
    "        bus_nonPg = torch.tensor(bus_nonPg).to(device)\n",
    "        bus_nonQg = torch.tensor(bus_nonQg).to(device)\n",
    "\n",
    "        # gencost_tensor, idxPg_tensor, baseMVA_tensor = gencost_tensor.to(device), idxPg_tensor.to(device), baseMVA_tensor.to(device)\n",
    "        # rev\n",
    "        gencost_tensor = gencost_tensor.to(device)\n",
    "        kgenp_max, kgenq_max, kpd_max, kqd_max = kgenp_max.to(device), kgenq_max.to(device), kpd_max.to(device), kqd_max.to(device)\n",
    "        kcost, kpd, kqd, kgenp, kgenq = kcost.to(device), kpd.to(device), kqd.to(device), kgenp.to(device), kgenq.to(device)\n",
    "        kv, kv_max = kv.to(device),kv_max.to(device)\n",
    "        VmUb, VmLb = VmUb.to(device), VmLb.to(device)\n",
    "        \n",
    "    \n",
    "\n",
    "    print('*' * 5+'training'+'*' * 5)\n",
    "    #Training process: Voltage magnitude\n",
    "    start_time = time.process_time()\n",
    "    for epoch in range(Epoch):\n",
    "        running_loss = 0.0\n",
    "        for step, (train_x, train_y) in enumerate(training_loader_v):\n",
    "            #feedforward\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            yvtrain_hat = model(train_x)   \n",
    "            #  supervised learning \n",
    "            loss = MSE_loss(train_y, yvtrain_hat) # with no penalty\n",
    "            # loss = kpy*MSE_loss(train_y, yvtrain_hat) + func_loss.apply(yvtrain_hat, train_x)  # with penalty\n",
    "            \n",
    "            # # unsupervised learning\n",
    "            # loss = func_loss.apply(yvtrain_hat, train_x)  # with penalty\n",
    "            \n",
    "            running_loss =  running_loss + loss.item()\n",
    "            # print('train_x',train_x.shape,'train_y', train_y.shape,'yvtrain_hat', yvtrain_hat.shape)\n",
    "\n",
    "            # backproprogate\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()      \n",
    "\n",
    "        loss_log.append(running_loss/len(training_loader_v))\n",
    "\n",
    "        if (epoch+1)%p_epoch == 0:          \n",
    "#             print('epoch', epoch+1, round(running_loss,4)/len(training_loader_v), \\\n",
    "#                   'Vm',torch.min(yvtrain_hat[:, NPred_Va:]).detach(),torch.max(yvtrain_hat[:, NPred_Va:]).detach(), \\\n",
    "#                   'real',torch.min(train_y[:, NPred_Va:]).detach(),torch.max(train_y[:, NPred_Va:]).detach(), \\\n",
    "#                   'mae', torch.mean(torch.abs(train_y[:, NPred_Va:]-yvtrain_hat[:,NPred_Va:])).detach(), \\\n",
    "#                   'Va',torch.min(yvtrain_hat[:, 0:NPred_Va]).detach(),torch.max(yvtrain_hat[:, 0:NPred_Va]).detach(), \\\n",
    "#                   'real',torch.min(train_y[:, 0:NPred_Va]).detach(),torch.max(train_y[:, 0:NPred_Va]).detach(), \\\n",
    "#                   'mae', torch.mean(torch.abs(train_y[:, 0:NPred_Va]-yvtrain_hat[:, 0:NPred_Va])).detach())    \n",
    "            \n",
    "            print('epoch', epoch+1, running_loss/len(training_loader_v), \\\n",
    "                  'Va',torch.min(yvtrain_hat[:, 0:NPred_Va]).detach(),torch.max(yvtrain_hat[:, 0:NPred_Va]).detach(), \\\n",
    "                  'Vm',torch.min(yvtrain_hat[:, NPred_Va:]).detach(),torch.max(yvtrain_hat[:, NPred_Va:]).detach())\n",
    "            \n",
    "            print('epoch', epoch+1, 'kcost', kcost, 'kgenp', torch.mean(kgenp), 'kgenq', torch.mean(kgenq), \\\n",
    "                  'kpd', torch.mean(kpd), 'kqd', torch.mean(kqd),'kv', torch.mean(kv),'\\n')\n",
    "\n",
    "        if (epoch+1)%100 == 0 and (epoch+1) >= s_epoch:\n",
    "            # save trianed model\n",
    "            torch.save(model.state_dict(), PATHs+'E'+str(epoch+1)+'.pth',_use_new_zipfile_serialization=False)\n",
    "\n",
    "    time_trian = time.process_time() - start_time  \n",
    "    print(\"\\n\")        \n",
    "    print('time_trian',np.round(time_trian,5),'seconds')\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # save trianed model\n",
    "    torch.save(model.state_dict(), PATH+'.pth',_use_new_zipfile_serialization=False)\n",
    "        \n",
    "    loss_log = np.array(loss_log)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(loss_log)\n",
    "    plt.title('loss')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(loss_log[Epoch-10:])\n",
    "    plt.title('loss') \n",
    "    plt.savefig(fignm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0ae87a-c665-4c0a-9837-7a36a5155ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load trained model: model_v_load\n",
      "yvmtest tensor([1.0500, 1.0467, 1.0327, 1.0295, 1.0331, 1.0242, 1.0206, 1.0147, 1.0272,\n",
      "        1.0289, 1.0272, 1.0333, 1.0500, 1.0254, 1.0278, 1.0258, 1.0238, 1.0182,\n",
      "        1.0152, 1.0179, 1.0337, 1.0382, 1.0406, 1.0315, 1.0386, 1.0243, 1.0500,\n",
      "        1.0258, 1.0344, 1.0252])\n",
      "yvatest tensor([ 0.0000, -0.0106, -0.0310, -0.0369, -0.0335, -0.0429, -0.0461, -0.0490,\n",
      "        -0.0578, -0.0656, -0.0578, -0.0621, -0.0460, -0.0705, -0.0686, -0.0677,\n",
      "        -0.0691, -0.0777, -0.0805, -0.0775, -0.0654, -0.0637, -0.0577, -0.0608,\n",
      "        -0.0473, -0.0533, -0.0352, -0.0443, -0.0519, -0.0637])\n",
      "before Kron reduction\n",
      "yvmtest_hat tensor([1.0500, 1.0467, 1.0327, 1.0294, 0.0000, 0.0000, 1.0205, 1.0146, 0.0000,\n",
      "        1.0288, 0.0000, 1.0332, 1.0500, 1.0254, 1.0278, 1.0257, 1.0238, 1.0182,\n",
      "        1.0152, 1.0179, 1.0336, 1.0382, 1.0405, 1.0315, 0.0000, 1.0246, 1.0500,\n",
      "        0.0000, 1.0343, 1.0252])\n",
      "yvatest_hat tensor([ 0.0000, -0.0698, -0.0698, -0.0698,  0.0000,  0.0000, -0.0698, -0.0698,\n",
      "         0.0000, -0.0698,  0.0000, -0.0698, -0.0698, -0.0705, -0.0698, -0.0698,\n",
      "        -0.0699, -0.0779, -0.0806, -0.0776, -0.0698, -0.0698, -0.0698, -0.0698,\n",
      "         0.0000, -0.0698, -0.0698,  0.0000, -0.0698, -0.0698])\n",
      "after Kron reduction\n",
      "yvmtest_hat tensor([1.0500, 1.0467, 1.0327, 1.0294, 1.0319, 1.0246, 1.0205, 1.0146, 1.0274,\n",
      "        1.0288, 1.0274, 1.0332, 1.0500, 1.0254, 1.0278, 1.0257, 1.0238, 1.0182,\n",
      "        1.0152, 1.0179, 1.0336, 1.0382, 1.0405, 1.0315, 1.0385, 1.0246, 1.0500,\n",
      "        1.0259, 1.0343, 1.0252])\n",
      "yvatest_hat tensor([ 0.0000, -0.0698, -0.0698, -0.0698, -0.0712, -0.0701, -0.0698, -0.0698,\n",
      "        -0.0699, -0.0698, -0.0699, -0.0698, -0.0698, -0.0705, -0.0698, -0.0698,\n",
      "        -0.0699, -0.0779, -0.0806, -0.0776, -0.0698, -0.0698, -0.0698, -0.0698,\n",
      "        -0.0702, -0.0698, -0.0698, -0.0709, -0.0698, -0.0698])\n",
      "mae_Vmtest tensor(0.0005)\n",
      "mre_Vmtest tensor(0.0482) %   tensor(3.1579) %\n",
      "mae_Vatest tensor(0.0157)\n",
      "mre_Vatest tensor(52.3191) %   tensor(1497.2327) %\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# testing Va using training data\n",
    "if flag_test: \n",
    "    # load trained model \n",
    "    print('load trained model: model_v_load')\n",
    "    model = NetV(input_channels, output_channels, hidden_units, khidden, Vscale, Vbias)\n",
    "    model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "delta_time = 0\n",
    "prediction = torch.zeros((Ntest, NPred_Vm+NPred_Va))  \n",
    "yvmtest_hat = torch.zeros((Ntest, Nbus))\n",
    "yvatest_hat = torch.zeros((Ntest, Nbus))\n",
    "for step,(test_x, test_y) in enumerate(test_loader_v):\n",
    "    if (step+1) % 1 == 0:\n",
    "        # print('test sample',step+1)   \n",
    "        test_x = test_x.to(device)\n",
    "        start_time = time.process_time()\n",
    "        prediction[step] = model(test_x).detach().cpu()\n",
    "        delta_time += time.process_time() - start_time\n",
    "\n",
    "\n",
    "xam_P = torch.from_numpy(np.insert(prediction.numpy(), idx_bus_Pnet_slack, 0, axis=1)) # insert slack bus fi = 0\n",
    "yvatest_hat[:, bus_Pnet_all.tolist()] = xam_P[:, 0:NPred_Vm]\n",
    "yvmtest_hat[:, bus_Pnet_all.tolist()] = xam_P[:, NPred_Vm:]\n",
    "\n",
    "time_NN = delta_time\n",
    "\n",
    "print('yvmtest',yvmtest[0])\n",
    "print('yvatest',yvatest[0])\n",
    "print('before Kron reduction')\n",
    "print('yvmtest_hat',yvmtest_hat[0])\n",
    "print('yvatest_hat',yvatest_hat[0])\n",
    "\n",
    "# recover ZIB\n",
    "Vetest_ZIB_hat = np.zeros((Ntest, bus_ZIB_all.shape[0]))\n",
    "Vftest_ZIB_hat = np.zeros((Ntest, bus_ZIB_all.shape[0]))\n",
    "delta_time = 0\n",
    "Vx_hat = yvmtest_hat[:, bus_Pnet_all.tolist()].numpy()*np.exp(1j*yvatest_hat[:, bus_Pnet_all.tolist()].numpy())\n",
    "for i in range(Ntest):\n",
    "    start_time = time.process_time()\n",
    "    Vy_hat = np.dot(param_ZIMV, Vx_hat[i])\n",
    "    Vetest_ZIB_hat[i] = Vy_hat.real\n",
    "    Vftest_ZIB_hat[i] = Vy_hat.imag\n",
    "    delta_time += time.process_time() - start_time\n",
    "\n",
    "time_ZIB = delta_time\n",
    "Vmy, Vay = cart2pol(Vetest_ZIB_hat, Vftest_ZIB_hat)   \n",
    "yvmtest_hat[:, bus_ZIB_all] = torch.from_numpy(Vmy).float()\n",
    "yvatest_hat[:, bus_ZIB_all] = torch.from_numpy(Vay).float()\n",
    "yvmtest_hat[:, bus_ZIB_all] = get_clamp(yvmtest_hat[:, bus_ZIB_all], VmLb[0].item(), VmUb[0].item())\n",
    "\n",
    "print('after Kron reduction')\n",
    "print('yvmtest_hat',yvmtest_hat[0])\n",
    "print('yvatest_hat',yvatest_hat[0])\n",
    "\n",
    "mae_Vmtest = get_mae(yvmtest, yvmtest_hat)\n",
    "mre_Vmtest = get_rerr(yvmtest,yvmtest_hat)\n",
    "print('mae_Vmtest', mae_Vmtest)\n",
    "print('mre_Vmtest', torch.mean(mre_Vmtest),'%  ',  torch.max(mre_Vmtest),'%')\n",
    "\n",
    "mae_Vatest = get_mae(yvatest[:,bus_Va], yvatest_hat[:,bus_Va])\n",
    "mre_Vatest = get_rerr(yvatest[:,bus_Va],yvatest_hat[:,bus_Va])\n",
    "print('mae_Vatest', mae_Vatest)\n",
    "print('mre_Vatest', torch.mean(mre_Vatest),'%  ',  torch.max(mre_Vatest),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31492ed1-b269-47ac-a954-3f80169e6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "## violation calculation function\n",
    "# Pg Qg violation\n",
    "def get_vioPQg(Pred_Pg, bus_Pg, MAXMIN_Pg, Pred_Qg, bus_Qg, MAXMIN_Qg):\n",
    "    vio_PQgmaxminnum = torch.zeros((Pred_Pg.shape[0],4))\n",
    "    vio_PQgmaxmin = torch.zeros((Pred_Pg.shape[0],4))\n",
    "    vio_PQg = torch.zeros((Pred_Pg.shape[0],2))\n",
    "    lsPg = list()\n",
    "    lsQg = list()\n",
    "    lsidxPg = np.zeros((Pred_Pg.shape[0]),dtype = np.int32) # index of sample that has violation\n",
    "    lsidxQg = np.zeros((Pred_Pg.shape[0]),dtype = np.int32) # index of sample that has violation\n",
    "    kP = 1# inital=0: in order to find nonzero samples \n",
    "    kQ = 1\n",
    "    deltaPgL = np.array([[0, 0]])\n",
    "    deltaPgU = np.array([[0, 0]])\n",
    "    deltaQgL = np.array([[0, 0]])\n",
    "    deltaQgU = np.array([[0, 0]])\n",
    "    for i in range(Pred_Pg.shape[0]):\n",
    "        # P\n",
    "        delta = Pred_Pg[i] - MAXMIN_Pg[:, 0]\n",
    "        idxPgUB = np.array(np.where(delta > DELTA))\n",
    "        if np.size(idxPgUB) > 0:\n",
    "            PgUB = np.concatenate((idxPgUB,delta[idxPgUB]),axis=0).T \n",
    "            deltaPgU = np.append(deltaPgU, PgUB, axis = 0)\n",
    "#             print('idxPgUB',idxPgUB,Pred_Pg[i,idxPgUB],MAXMIN_Pg[idxPgUB, 0],delta[idxPgUB])\n",
    "\n",
    "        delta = Pred_Pg[i] - MAXMIN_Pg[:, 1]\n",
    "        idxPgLB = np.array(np.where(delta < -DELTA))\n",
    "        if np.size(idxPgLB) > 0:\n",
    "            PgLB = np.concatenate((idxPgLB,delta[idxPgLB]),axis=0).T\n",
    "            deltaPgL = np.append(deltaPgL, PgLB, axis = 0)\n",
    "        \n",
    "        if np.size(idxPgUB)>0 and np.size(idxPgLB) > 0:\n",
    "            PgLUB = np.concatenate((PgUB,PgLB),axis=0)\n",
    "        elif np.size(idxPgUB) > 0:\n",
    "            PgLUB = PgUB\n",
    "        elif np.size(idxPgLB) > 0:\n",
    "            PgLUB = PgLB\n",
    "        \n",
    "        if (np.size(idxPgUB) + np.size(idxPgLB)) > 0:\n",
    "            PgLUB = PgLUB[PgLUB[:,0].argsort()]\n",
    "            lsPg.append(PgLUB)\n",
    "            lsidxPg[i] = kP # has violation:index of lsPg \n",
    "            kP+= 1#index=lsPg\n",
    "\n",
    "        # Q\n",
    "        delta = Pred_Qg[i] - MAXMIN_Qg[:, 0]\n",
    "        idxQgUB = np.array(np.where(delta > DELTA))\n",
    "        if np.size(idxQgUB) > 0:\n",
    "            QgUB = np.concatenate((idxQgUB,delta[idxQgUB]),axis=0).T\n",
    "            deltaQgU = np.append(deltaQgU, QgUB, axis = 0)\n",
    "\n",
    "        delta = Pred_Qg[i] - MAXMIN_Qg[:, 1]\n",
    "        idxQgLB = np.array(np.where(delta < -DELTA))\n",
    "        if np.size(idxQgLB) > 0:\n",
    "            QgLB = np.concatenate((idxQgLB,delta[idxQgLB]),axis=0).T\n",
    "            deltaQgL = np.append(deltaQgL, QgLB, axis = 0)\n",
    "            \n",
    "        if np.size(idxQgUB) > 0 and np.size(idxQgLB) > 0:   \n",
    "            QgLUB = np.concatenate((QgUB,QgLB),axis=0)\n",
    "        elif np.size(idxQgUB) > 0:\n",
    "            QgLUB = QgUB\n",
    "        elif np.size(idxQgLB) > 0:\n",
    "            QgLUB = QgLB\n",
    "         \n",
    "        if (np.size(idxQgUB) + np.size(idxQgLB)) > 0:\n",
    "            QgLUB = QgLUB[QgLUB[:,0].argsort()]\n",
    "            lsQg.append(QgLUB)\n",
    "            lsidxQg[i] = kQ # has violation\n",
    "            kQ+= 1\n",
    "                    \n",
    "        \n",
    "        vio_PQgmaxminnum[i,0] = np.size(idxPgUB)\n",
    "        vio_PQgmaxminnum[i,1] = np.size(idxPgLB)\n",
    "        vio_PQgmaxminnum[i,2] = np.size(idxQgUB)\n",
    "        vio_PQgmaxminnum[i,3] = np.size(idxQgLB)\n",
    "        \n",
    "    # Pg Qg violation ratio\n",
    "    vio_PQgmaxmin[:,0] = (1 - vio_PQgmaxminnum[:, 0]/bus_Pg.shape[0])*100\n",
    "    vio_PQgmaxmin[:,1] = (1 - vio_PQgmaxminnum[:, 1]/bus_Pg.shape[0])*100\n",
    "    vio_PQgmaxmin[:,2] = (1 - vio_PQgmaxminnum[:, 2]/bus_Qg.shape[0])*100\n",
    "    vio_PQgmaxmin[:,3] = (1 - vio_PQgmaxminnum[:, 3]/bus_Qg.shape[0])*100\n",
    "    vio_PQg[:, 0] = (1 - (vio_PQgmaxminnum[:, 0] + vio_PQgmaxminnum[:, 1])/bus_Pg.shape[0])*100\n",
    "    vio_PQg[:, 1] = (1 - (vio_PQgmaxminnum[:, 2] + vio_PQgmaxminnum[:, 3])/bus_Qg.shape[0])*100\n",
    "     \n",
    "    # delete initial \n",
    "    if deltaPgL.shape[0] > 1:\n",
    "        deltaPgL = np.delete(deltaPgL, 0, axis = 0)\n",
    "        \n",
    "    if deltaPgU.shape[0] > 1:    \n",
    "        deltaPgU = np.delete(deltaPgU, 0, axis = 0)\n",
    "    \n",
    "    if deltaQgL.shape[0] > 1:\n",
    "        deltaQgL = np.delete(deltaQgL, 0, axis = 0)\n",
    "    \n",
    "    if deltaQgU.shape[0] > 1:\n",
    "        deltaQgU = np.delete(deltaQgU, 0, axis = 0)\n",
    "    \n",
    "    return lsPg,lsQg,lsidxPg,lsidxQg,vio_PQgmaxmin,vio_PQg, deltaPgL,deltaPgU,deltaQgL,deltaQgU\n",
    "\n",
    "\n",
    "# branch constraints with violation details for dV\n",
    "def get_viobran2(Pred_V,Pred_Va,branch,Yf,Yt):\n",
    "   #branch=[branch from bus, branch to bus, branch limit, minang, maxang]\n",
    "    branlp = branch[:, 2]/baseMVA\n",
    "    angminmax = branch[:, 3:5]*math.pi/180\n",
    "    Pred_branang = Pred_Va[:, BRANFT[:, 0]] - Pred_Va[:, BRANFT[:, 1]]\n",
    "    vio_branangnum = torch.zeros(Pred_V.shape[0])\n",
    "    vio_branpfnum = torch.zeros(Pred_V.shape[0])\n",
    "    vio_branpfidx = torch.zeros(Pred_V.shape[0])\n",
    "    lsSf = []\n",
    "    lsSt = []\n",
    "    lsSf_sampidx = []\n",
    "    lsSt_sampidx = []\n",
    "    deltapf = np.array([[0, 0]])\n",
    "    for i in range(Pred_V.shape[0]):\n",
    "    #for i in range(1):\n",
    "        vio_branangnum[i] = np.size(np.where(Pred_branang[i, :] - angminmax[:,0] < -DELTA)) \\\n",
    "        + np.size(np.where(Pred_branang[i, :] - angminmax[:,1] > DELTA))\n",
    "\n",
    "        # branch power flow\n",
    "        fV = Pred_V[i, BRANFT[:, 0]]\n",
    "        tV = Pred_V[i, BRANFT[:, 1]]        \n",
    "        fI = Yf.dot(Pred_V[i]).conj()\n",
    "        tI = Yt.dot(Pred_V[i]).conj()\n",
    "        fS = np.multiply(fV, fI)\n",
    "        tS = np.multiply(tV, tI)\n",
    "        deltafS = np.abs(fS) - branlp\n",
    "        deltatS = np.abs(tS) - branlp\n",
    "        deltafS = np.array(deltafS).ravel()\n",
    "        deltatS = np.array(deltatS).ravel()\n",
    "        idxfs = np.array(np.where(deltafS > DELTA)).reshape(-1, 1)\n",
    "        idxts = np.array(np.where(deltatS > DELTA)).reshape(-1, 1)\n",
    "        vio_branpfnum[i] = np.size(idxfs) + np.size(idxfs)\n",
    "        if np.size(idxfs) >= 1:           \n",
    "            ii = np.concatenate((idxfs,deltafS[idxfs]),axis=1)  \n",
    "            deltapf = np.append(deltapf, ii, axis = 0) \n",
    "            ii = np.concatenate((ii,np.real(fS[idxfs]),np.imag(fS[idxfs])),axis=1)\n",
    "            lsSf.append(ii)\n",
    "            lsSf_sampidx.append(i)           \n",
    "            \n",
    "        if np.size(idxts) >= 1:\n",
    "            ii = np.concatenate((idxts,deltatS[idxts]),axis=1)\n",
    "            deltapf = np.append(deltapf, ii, axis = 0) \n",
    "            # print('ii',ii.shape,'tS',tS.shape,'idxts',idxts.shape)\n",
    "            ii = np.concatenate((ii,np.real(tS[idxts]),np.imag(tS[idxts])),axis=1)\n",
    "            lsSt.append(ii)\n",
    "            lsSt_sampidx.append(i)\n",
    "\n",
    "        if np.size(idxfs) + np.size(idxts) >= 1:\n",
    "            vio_branpfidx[i] = i+1\n",
    "            \n",
    "    # delete initial\n",
    "    if deltapf.shape[0] > 1:\n",
    "        deltapf = np.delete(deltapf, 0, axis = 0)\n",
    "        deltapfR = deltapf[:, 1]/branlp[0, deltapf[:, 0].astype(int)]*100\n",
    "        deltapf = np.insert(deltapf, 2, values=deltapfR, axis=1)\n",
    " \n",
    "    vio_branang = (1-vio_branangnum/branch.shape[0])*100 \n",
    "    vio_branpf = (1-vio_branpfnum/(branch.shape[0]*2))*100\n",
    "    return vio_branang,vio_branpf,deltapf,vio_branpfidx,lsSf,lsSt,lsSf_sampidx,lsSt_sampidx\n",
    "\n",
    "def dSlbus_dV(his_V,bus_Va):\n",
    "    V = his_V.copy()\n",
    "    # branch power flow for FROM branch\n",
    "    fV = V[BRANFT[:, 0]]\n",
    "    fI = Yf.dot(V).conj()\n",
    "    Nsam = 1\n",
    "    dfP_dVm = np.zeros((Nsam, branch.shape[0], Nbus))   \n",
    "    dfQ_dVm = np.zeros((Nsam, branch.shape[0], Nbus))\n",
    "    dfP_dVa = np.zeros((Nsam, branch.shape[0], Nbus))    \n",
    "    dfQ_dVa = np.zeros((Nsam, branch.shape[0], Nbus))                \n",
    "    diagfI = np.diag(fI)\n",
    "    diagfV = np.diag(fV)\n",
    "    diagVnorm = np.diag(np.true_divide(V, np.abs(V)))\n",
    "#     print('diagfV',diagfV.shape,'diagVnorm',diagVnorm.shape,'Yf',Yf.shape,'diagfI',diagfI.shape,'finc',finc.shape)\n",
    "    dfS_dVm = np.dot(diagfV, Yf.dot(diagVnorm).conj()) + np.dot(diagfI.conj(), np.dot(finc,diagVnorm))\n",
    "    dfP_dVm = np.real(dfS_dVm)\n",
    "    dfQ_dVm = np.imag(dfS_dVm)\n",
    "    diagV = np.diag(V)\n",
    "    dfS_dVa = -1j*np.dot(diagfV, Yf.dot(diagV).conj()) + 1j*np.dot(diagfI.conj(), np.dot(finc, diagV))\n",
    "    dfP_dVa = np.real(dfS_dVa)\n",
    "    dfQ_dVa = np.imag(dfS_dVa)\n",
    "    dPfbus_dV = np.concatenate((dfP_dVa, dfP_dVm), axis = 1)\n",
    "    dQfbus_dV = np.concatenate((dfQ_dVa, dfQ_dVm), axis = 1)\n",
    "\n",
    "    return dPfbus_dV, dQfbus_dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7e4a6cf-7f84-4d31-8ded-0b410af665a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revision\n",
    "# modify V theta\n",
    "# dPbus_dV dQbus_dV\n",
    "def dPQbus_dV(his_V,bus_Pg,bus_Qg):\n",
    "    V = his_V.copy()\n",
    "#     Ibus = np.dot(Ybus, his_V)\n",
    "    Ibus = Ybus.dot(his_V).conj()\n",
    "    diagV = np.diag(V)\n",
    "    diagIbus = np.diag(Ibus)\n",
    "    diagVnorm = np.diag(V/np.abs(V))\n",
    "    dSbus_dVm = np.dot(diagV, Ybus.dot(diagVnorm).conj()) + np.dot(diagIbus.conj(), diagVnorm)\n",
    "    dSbus_dVa = 1j*np.dot(diagV, (diagIbus - Ybus.dot(diagV)).conj())\n",
    "    dSbus_dV = np.concatenate((dSbus_dVa, dSbus_dVm), axis=1)\n",
    "    dPbus_dV = np.real(dSbus_dV)\n",
    "    dQbus_dV = np.imag(dSbus_dV)\n",
    "    \n",
    "    return dPbus_dV,dQbus_dV\n",
    "\n",
    "# calculate dV\n",
    "def get_hisdV(lsPg,lsQg,lsidxPg,lsidxQg,num_viotest,k_dV,bus_Pg,bus_Qg,dPbus_dV,dQbus_dV):\n",
    "    dV = np.zeros((num_viotest,Nbus*2))\n",
    "    j = 0\n",
    "    for i in range(Ntest):\n",
    "        # determin whether there is violation\n",
    "        if (lsidxPg[i] + lsidxQg[i]) > 0:\n",
    "            # calculate dS_dV, dPbus_dV,dQbus_dV\n",
    "            # get BusPg\n",
    "            if lsidxPg[i] > 0 and lsidxQg[i] > 0:\n",
    "                idxPg = lsPg[lsidxPg[i]-1][:,0].astype(np.int32) #note: lsidxPg[i]-1\n",
    "                idxQg = lsQg[lsidxQg[i]-1][:,0].astype(np.int32) #note: lsidxQg[i]-1\n",
    "                busPg = bus_Pg[idxPg]\n",
    "                busQg = bus_Qg[idxQg]\n",
    "                dPGbus_dV = dPbus_dV[busPg, :]\n",
    "                dQGbus_dV = dQbus_dV[busQg, :]\n",
    "\n",
    "                if busPg.shape[0] == 1:\n",
    "                    dPGbus_dV = dPGbus_dV.reshape(1, -1)\n",
    "                    \n",
    "                if busQg.shape[0] == 1:\n",
    "                    dQGbus_dV = dQGbus_dV.reshape(1, -1)               \n",
    "                \n",
    "                dPQGbus_dV = np.concatenate((dPGbus_dV, dQGbus_dV), axis=0) #need bus number pf Pg Qg\n",
    "                dPQg = np.concatenate((lsPg[lsidxPg[i]-1][:,1], lsQg[lsidxQg[i]-1][:,1]), axis=0)\n",
    "            elif lsidxPg[i] > 0:  \n",
    "                idxPg = lsPg[lsidxPg[i]-1][:,0].astype(np.int32)\n",
    "                busPg = bus_Pg[idxPg]\n",
    "                dPQGbus_dV = dPbus_dV[busPg, :]\n",
    "                dPQg = lsPg[lsidxPg[i]-1][:,1]\n",
    "            elif lsidxQg[i] > 0:     \n",
    "                # get BusQg\n",
    "                idxQg = lsQg[lsidxQg[i]-1][:,0].astype(np.int32)\n",
    "                busQg = bus_Qg[idxQg]\n",
    "                dPQGbus_dV = dQbus_dV[busQg, :]\n",
    "                dPQg = lsQg[lsidxQg[i]-1][:,1]\n",
    "\n",
    "            if dPQGbus_dV.ndim > 1:\n",
    "                dV[j] = np.dot(np.linalg.pinv(dPQGbus_dV), dPQg*k_dV)    \n",
    "            else:\n",
    "                idx_nonzero = np.where(dPQGbus_dV > 1e-4)\n",
    "                dV[j, idx_nonzero] = dPQg*k_dV/dPQGbus_dV[idx_nonzero]\n",
    "                \n",
    "            j+=1 \n",
    "            \n",
    "    return dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379db36e-64f8-4c4f-87fe-6434727fc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramters load back to CPU\n",
    "if torch.cuda.is_available():\n",
    "    MAXMIN_Pg_tensor = MAXMIN_Pg_tensor.cpu()\n",
    "    MAXMIN_Qg_tensor = MAXMIN_Qg_tensor.cpu()\n",
    "    \n",
    "    if flag_test==0:\n",
    "        bus_Pg,bus_Qg = bus_Pg.cpu(),bus_Qg.cpu()\n",
    "        bus_Pd,bus_Qd = bus_Pd.cpu(),bus_Qd.cpu()\n",
    "        bus_nonPg, bus_nonQg = bus_nonPg.cpu(), bus_nonQg.cpu()\n",
    "\n",
    "    gencost_tensor = gencost_tensor.cpu()\n",
    "    # rev\n",
    "    kgenp_max, kgenq_max, kpd_max, kqd_max = kgenp_max.cpu(), kgenq_max.cpu(), kpd_max.cpu(), kqd_max.cpu()\n",
    "    kcost, kpd, kqd, kgenp, kgenq = kcost.cpu(), kpd.cpu(), kqd.cpu(), kgenp.cpu(), kgenq.cpu()\n",
    "    kv, kv_max = kv.cpu(),kv_max.cpu()\n",
    "    VmUb, VmLb = VmUb.cpu(), VmLb.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f83381c5-6339-4ee6-8281-8803d19ba3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mre_cost (10000,) mean\u001b[1;31m  -31.53 \u001b[0m%  max -22.32 %  min -40.36 %\n",
      "first time: mre_Pd mean\u001b[1;31m  60.71 \u001b[0m%  max 64.81 % mre_Qd mean\u001b[1;31m  76.23 \u001b[0m%  max 95.31 %\n",
      "vio_PQg mean tensor(67.3267) min tensor(66.6667) max tensor(83.3333) %  num_viotest 10000 Pg mean\u001b[1;31m  66.67  \u001b[0m% Qg mean\u001b[1;31m  67.99 \u001b[0m%\n",
      "first time: vio_branang: mean\u001b[1;31m  100.0 \u001b[0m% max 100.0 %  first time: vio_branpf: mean\u001b[1;31m  100.0 \u001b[0m% max 100.0 % \n",
      "first time: res_branpf  [0.001  0.002  0.3136 0.6381] res_PQUL [-0.8738 -0.922   0.7598  0.7844 -0.1208 -0.199   0.0957  0.1551]\n",
      "***** revision *****\n",
      "his_V (30,) <class 'numpy.ndarray'>\n",
      "dPbus_dV (30, 60) dQbus_dV (30, 60)\n",
      "dPfbus_dV (41, 60) dQfbus_dV (41, 60)\n",
      "revision_time_branch each 0.0\n",
      "revision_time_bus each 0.0003125\n",
      "mae_Vatest1 0.015111748 mre_Vatest: min 0.0 max 1497.2327 mean 51.776474\n",
      "mae_Vmtest1 0.000491954 mre_Vmtest1: min 0.0 max 3.157898 mean 0.05004616\n",
      "mre_cost1 (10000,) mean\u001b[1;31m  -30.72 \u001b[0m%  max -21.1 %  min -39.56 %\n",
      "after revision: mre_Pd1 mean\u001b[1;31m  62.17 \u001b[0m%  max 66.82 % mre_Qd1 mean\u001b[1;31m  76.18  \u001b[0m%  max 95.8 %\n",
      "vio_PQg1 mean tensor(67.3267) min tensor(66.6667) max tensor(83.3333) %  num_viotest 10000 Pg mean\u001b[1;31m  66.67  \u001b[0m% Qg mean\u001b[1;31m  67.9867 \u001b[0m%\n",
      "after revision: vio_branang1: mean\u001b[1;31m  100.0 \u001b[0m% max 100.0 %  after revision: vio_branpf1: mean\u001b[1;31m  100.0 \u001b[0m% max 100.0 % \n",
      "after revision: res_branpf1  [0.001  0.002  0.3136 0.6381] res_PQUL1 [-0.8224 -0.8662  0.7294  0.7532 -0.124  -0.1907  0.0965  0.1501]\n",
      "total_time_each 0.001098\n",
      "total_time_each_rev 0.001411\n",
      "optimality_loss 31.526338958185747 %\n"
     ]
    }
   ],
   "source": [
    "# \\033[显示方式;前景色;背景色m + 结尾部分：\\033[0m#\n",
    "# print('This is a \\033[1;31m test \\033[0m!')\n",
    "import time\n",
    "start_time = time.process_time()\n",
    "Pred_Vtest = yvmtest_hat.numpy() * np.exp(1j*yvatest_hat.numpy())\n",
    "Pred_Pgtest, Pred_Qgtest, Pred_Pdtest, Pred_Qdtest = get_genload(Pred_Vtest, Pdtest, Qdtest, bus_Pg, bus_Qg)\n",
    "time_cal = (time.process_time() - start_time)\n",
    "total_time_each = (time_cal + time_NN + time_ZIB)/Ntest\n",
    "    \n",
    "# prediction accuracy\n",
    "# real V, Pd Qg Pd Qd\n",
    "Real_Vtest = yvmtest.numpy()*np.exp(1j*yvatest.numpy())  \n",
    "Real_Pgtest, Real_Qgtest, Real_Pdtest, Real_Qdtest = get_genload(Real_Vtest, Pdtest, Qdtest, bus_Pg, bus_Qg) \n",
    "\n",
    "# optimality loss\n",
    "Pred_costtest = get_Pgcost(Pred_Pgtest,idxPg,gencost_Pg)\n",
    "Real_costtest = get_Pgcost(Real_Pgtest,idxPg,gencost_Pg)\n",
    "mre_cost = get_rerr2(Real_costtest,Pred_costtest)\n",
    "# print('Pred_costtest',Pred_costtest.shape, 'Real_costtest', Real_costtest.shape)\n",
    "# print('mre_cost', mre_cost.shape, 'mean',np.round(np.mean(mre_cost),4),'%  max', np.round(np.max(mre_cost),4), '%  min',np.round(np.min(mre_cost), 4),'%')\n",
    "print('mre_cost', mre_cost.shape, 'mean\\033[1;31m ',str(np.round(np.mean(mre_cost),2)), \\\n",
    "      '\\033[0m%  max', np.round(np.max(mre_cost),2), '%  min',np.round(np.min(mre_cost), 2),'%')\n",
    "\n",
    "\n",
    "# load satisfaction\n",
    "mre_Pd = 100 - get_rerr(Real_Pdtest.sum(axis=1), Pred_Pdtest.sum(axis=1))\n",
    "mre_Qd = 100 - get_rerr(Real_Qdtest.sum(axis=1), Pred_Qdtest.sum(axis=1))\n",
    "# mre_Pd = get_rerr(Real_Pdtest[:, bus_Pd].sum(axis=1), Pred_Pdtest[:, bus_Pd].sum(axis=1))\n",
    "# mre_Qd = get_rerr(Real_Qdtest[:, bus_Qd].sum(axis=1), Pred_Qdtest[:, bus_Qd].sum(axis=1))\n",
    "# print('first time: mre_Pd mean', np.round(np.mean(mre_Pd), 4),'%  max', np.round(np.max(mre_Pd),4),'%', \\\n",
    "#      'mre_Qd mean', np.round(np.mean(mre_Qd), 4),'%  max', np.round(np.max(mre_Qd), 4),'%') \n",
    "print('first time: mre_Pd mean\\033[1;31m ',str(np.round(np.mean(mre_Pd),2)),'\\033[0m%  max', np.round(np.max(mre_Pd),2),'%', \\\n",
    "     'mre_Qd mean\\033[1;31m ',str(np.round(np.mean(mre_Qd),2)),'\\033[0m%  max', np.round(np.max(mre_Qd), 2),'%') \n",
    "\n",
    "\n",
    "#calculate violation\n",
    "# Pg Qg violation\n",
    "lsPg,lsQg,lsidxPg,lsidxQg,vio_PQgmaxmin,vio_PQg,deltaPgL,deltaPgU,deltaQgL,deltaQgU = get_vioPQg(Pred_Pgtest, bus_Pg, MAXMIN_Pg, Pred_Qgtest, bus_Qg, MAXMIN_Qg)\n",
    "lsidxPQg = np.squeeze(np.array(np.where((lsidxPg + lsidxQg) > 0)))\n",
    "num_viotest = np.size(lsidxPQg)\n",
    "# print('vio_PQg max', torch.max(vio_PQg), 'min', torch.min(vio_PQg), 'mean', torch.mean(vio_PQg), '%',' num_viotest',str(num_viotest),'mean P Q',torch.mean(vio_PQg, 0))\n",
    "print('vio_PQg mean', torch.mean(vio_PQg), 'min', torch.min(vio_PQg), 'max', torch.max(vio_PQg), '%', \\\n",
    "      ' num_viotest',str(num_viotest),'Pg mean\\033[1;31m ',str(np.round(np.mean(vio_PQg[:, 0].numpy()),2)), ' \\033[0m%', \\\n",
    "      'Qg mean\\033[1;31m ',str(np.round(np.mean(vio_PQg[:, 1].numpy()),2)),'\\033[0m%')\n",
    "\n",
    "# revise power flow (from)\n",
    "vio_branang,vio_branpf,deltapf,vio_branpfidx,lsSf,_,lsSf_sampidx,_ = get_viobran2(Pred_Vtest,yvatest_hat.numpy(),branch,Yf,Yt)\n",
    "vio_branpf_num = np.size(np.where(vio_branpfidx>0))\n",
    "lsSf_sampidx = np.asarray(lsSf_sampidx)\n",
    "# print('first time: vio_branang: mean', torch.mean(vio_branang), '%  max', torch.max(vio_branang),'% ', \\\n",
    "#       'first time: vio_branpf: mean',torch.mean(vio_branpf), '%  max', torch.max(vio_branpf),'% ')\n",
    "print('first time: vio_branang: mean\\033[1;31m ',str(np.round(np.mean(vio_branang.numpy()),2)), '\\033[0m% max', np.max(vio_branang.numpy()),'% ', \\\n",
    "      'first time: vio_branpf: mean\\033[1;31m ',str(np.round(np.mean(vio_branpf.numpy()),2)), '\\033[0m% max', np.max(vio_branpf.numpy()),'% ')\n",
    "\n",
    "res_PQUL = np.array((np.mean(deltaPgL[:,1]),np.min(deltaPgL[:,1]),np.mean(deltaPgU[:,1]),np.max(deltaPgU[:,1]), \\\n",
    "                        np.mean(deltaQgL[:,1]),np.min(deltaQgL[:,1]),np.mean(deltaQgU[:,1]),np.max(deltaQgU[:,1])))\n",
    "\n",
    "if deltapf.shape[1] > 2:\n",
    "    res_branpf = np.array((np.mean(deltapf[:,1]),np.max(deltapf[:,1]),np.mean(deltapf[:,2]),np.max(deltapf[:,2])))\n",
    "else:\n",
    "    res_branpf = np.array((np.mean(deltapf[:,1]),np.max(deltapf[:,1])))\n",
    "    \n",
    "print('first time: res_branpf ', res_branpf, 'res_PQUL', res_PQUL)   \n",
    "\n",
    "# historical V\n",
    "k_dV = 0.1\n",
    "print('*'*5,'revision','*'*5)\n",
    "print('his_V',his_V.shape,type(his_V))\n",
    "# for justification of Vm Va\n",
    "dPbus_dV,dQbus_dV = dPQbus_dV(his_V,bus_Pg,bus_Qg)\n",
    "print('dPbus_dV',dPbus_dV.shape, 'dQbus_dV', dQbus_dV.shape)\n",
    "dV1 = get_hisdV(lsPg,lsQg,lsidxPg,lsidxQg,num_viotest,k_dV,bus_Pg,bus_Qg,dPbus_dV,dQbus_dV)\n",
    "\n",
    "# branch\n",
    "dPfbus_dV, dQfbus_dV = dSlbus_dV(his_V,bus_Va)\n",
    "print('dPfbus_dV',dPfbus_dV.shape, 'dQfbus_dV', dQfbus_dV.shape)\n",
    "revision_time_branch = 0\n",
    "if vio_branpf_num > 0:\n",
    "    start_time = time.process_time()\n",
    "    dV_branch = np.zeros((lsSf_sampidx.shape[0], Nbus*2))      \n",
    "    for i in range(lsSf_sampidx.shape[0]):\n",
    "        # Pl/deltaSl\n",
    "        mp = np.array(lsSf[i][:, 2]/lsSf[i][:, 1]).reshape(-1, 1)\n",
    "        mq = np.array(lsSf[i][:, 3]/lsSf[i][:, 1]).reshape(-1, 1)\n",
    "        # dPf_dVaVm\n",
    "        dPdV = dPfbus_dV[np.array(lsSf[i][:, 0].astype(int)).squeeze(), :] \n",
    "        # dQf_dVaVm\n",
    "        dQdV = dQfbus_dV[np.array(lsSf[i][:, 0].astype(int)).squeeze(), :]\n",
    "        dmp = mp*dPdV \n",
    "        dmq = mq*dQdV\n",
    "        # M: dS_dVaVm\n",
    "        dmpq_inv = np.linalg.pinv(dmp+dmq)\n",
    "        dV_branch[i] = np.dot(dmpq_inv, np.array(lsSf[i][:, 1])).squeeze() \n",
    "        \n",
    "    revision_time_branch = (time.process_time() - start_time)/Ntest\n",
    "    print('revision_time_branch each',revision_time_branch)\n",
    "\n",
    "start_time = time.process_time()\n",
    "Pred_Vm1 = yvmtest_hat.clone().numpy()\n",
    "Pred_Va1 = yvatest_hat.clone().numpy()\n",
    "# if lsidxPQg.shape[0] > 0: \n",
    "if  np.sum(lsidxPQg) > 0: \n",
    "    Pred_Va1[lsidxPQg, :] = yvatest_hat[lsidxPQg, :].numpy() - dV1[:, 0:Nbus] # Pg Qg violation\n",
    "    Pred_Vm1[lsidxPQg, :] = yvmtest_hat[lsidxPQg, :].numpy() - dV1[:, Nbus:2*Nbus] # Pg Qg violation\n",
    "    \n",
    "if lsSf_sampidx.shape[0] > 0:    \n",
    "    Pred_Va1[lsSf_sampidx, :] = Pred_Va1[lsSf_sampidx, :] - dV_branch[:, 0:Nbus] # branch violation     \n",
    "    Pred_Vm1[lsSf_sampidx, :] = Pred_Vm1[lsSf_sampidx, :] - dV_branch[:, Nbus:2*Nbus] # branch violation\n",
    "    \n",
    "Pred_Va1[:,bus_slack] = 0 \n",
    "Pred_Vm1_clip = get_clamp(torch.from_numpy(Pred_Vm1), VmLb[0].item(), VmUb[0].item())\n",
    "Pred_Vtest1 = Pred_Vm1_clip.numpy() * np.exp(1j*Pred_Va1)\n",
    "Pred_Pgtest1, Pred_Qgtest1, Pred_Pdtest1, Pred_Qdtest1 = get_genload(Pred_Vtest1, Pdtest, Qdtest, bus_Pg, bus_Qg)\n",
    "revision_time_bus = (time.process_time() - start_time)/Ntest\n",
    "print('revision_time_bus each',revision_time_bus)\n",
    "\n",
    "###################################################################\n",
    "# prediction accuracy after revision\n",
    "mae_Vatest1 = get_mae(yvatest.numpy(), Pred_Va1)\n",
    "mre_Vatest1 = get_rerr(yvatest[:,bus_Va].numpy(), Pred_Va1[:,bus_Va])\n",
    "print('mae_Vatest1', mae_Vatest1,'mre_Vatest: min', np.min(mre_Vatest1), \\\n",
    "      'max', np.max(mre_Vatest1), 'mean', np.mean(mre_Vatest1))\n",
    "\n",
    "mae_Vmtest1 = get_mae(yvmtest.numpy(), Pred_Vm1_clip.numpy())\n",
    "mre_Vmtest1 = get_rerr(yvmtest.numpy(), Pred_Vm1_clip.numpy())\n",
    "print('mae_Vmtest1', mae_Vmtest1,'mre_Vmtest1: min', np.min(mre_Vmtest1), \\\n",
    "      'max', np.max(mre_Vmtest1), 'mean', np.mean(mre_Vmtest1))\n",
    "\n",
    "# optimality loss\n",
    "Pred_costtest1 = get_Pgcost(Pred_Pgtest1,idxPg,gencost_Pg)\n",
    "mre_cost1 = get_rerr2(Real_costtest,Pred_costtest1)\n",
    "# print('mre_cost1', mre_cost1.shape, 'mean',np.round(np.mean(mre_cost1),2),'%  max', np.round(np.max(mre_cost1),4), '%  min',np.round(np.min(mre_cost1), 2),'%')\n",
    "print('mre_cost1', mre_cost.shape, 'mean\\033[1;31m ',str(np.round(np.mean(mre_cost1),2)), \\\n",
    "      '\\033[0m%  max', np.round(np.max(mre_cost1),2), '%  min',np.round(np.min(mre_cost1), 2),'%')\n",
    "\n",
    "mre_Pd1 = 100 - get_rerr(Real_Pdtest.sum(axis=1), Pred_Pdtest1.sum(axis=1))\n",
    "mre_Qd1 = 100 - get_rerr(Real_Qdtest.sum(axis=1), Pred_Qdtest1.sum(axis=1))\n",
    "# mre_Pd1 = get_rerr(Real_Pdtest[:, bus_Pd].sum(axis=1), Pred_Pdtest1[:, bus_Pd].sum(axis=1))\n",
    "# mre_Qd1 = get_rerr(Real_Qdtest[:, bus_Qd].sum(axis=1), Pred_Qdtest1[:, bus_Qd].sum(axis=1))\n",
    "# print('after revision: mre_Pd1 mean', np.round(np.mean(mre_Pd1), 2),'%  max', np.round(np.max(mre_Pd1),2),'%', \\\n",
    "#      'mre_Qd1 mean', np.round(np.mean(mre_Qd1), 2),'%  max', np.round(np.max(mre_Qd1), 2),'%') \n",
    "print('after revision: mre_Pd1 mean\\033[1;31m ',str(np.round(np.mean(mre_Pd1),2)),'\\033[0m%  max', np.round(np.max(mre_Pd1),2),'%', \\\n",
    "     'mre_Qd1 mean\\033[1;31m ',str(np.round(np.mean(mre_Qd1),2)),' \\033[0m%  max', np.round(np.max(mre_Qd1), 2),'%') \n",
    "\n",
    "\n",
    "#calculate violation\n",
    "# Pg Qg violation\n",
    "_,_,lsidxPg1,lsidxQg1,vio_PQgmaxmin1,vio_PQg1,deltaPgL1,deltaPgU1,deltaQgL1,deltaQgU1 = get_vioPQg(Pred_Pgtest1, bus_Pg, MAXMIN_Pg, Pred_Qgtest1, bus_Qg, MAXMIN_Qg)\n",
    "lsidxPQg1 = np.squeeze(np.array(np.where((lsidxPg1 + lsidxQg1) > 0)))\n",
    "num_viotest1 = np.size(lsidxPQg1)\n",
    "# print('vio_PQg1 max', torch.max(vio_PQg1), 'min', torch.min(vio_PQg1), 'mean', torch.mean(vio_PQg1), '%',' num_viotest',str(num_viotest1),'mean P Q',torch.mean(vio_PQg1, 0))\n",
    "print('vio_PQg1 mean', torch.mean(vio_PQg1), 'min', torch.min(vio_PQg), 'max', torch.max(vio_PQg1), '%', \\\n",
    "      ' num_viotest',str(num_viotest1),'Pg mean\\033[1;31m ',str(np.round(np.mean(vio_PQg1[:, 0].numpy()),2)), \\\n",
    "      ' \\033[0m%','Qg mean\\033[1;31m ',str(np.round(np.mean(vio_PQg1[:, 1].numpy()),4)),'\\033[0m%')\n",
    "\n",
    "\n",
    "# branch\n",
    "vio_branang1,vio_branpf1,deltapf1,vio_branpfidx1,lsSf1,_,lsSf_sampidx1,_ = get_viobran2(Pred_Vtest1,yvatest_hat.numpy(),branch,Yf,Yt)\n",
    "# print('after revision: vio_branang1: mean', torch.mean(vio_branang1), '%  max', torch.max(vio_branang1),'% ', \\\n",
    "#       'vio_branpf1: mean',torch.mean(vio_branpf1), '%  max', torch.max(vio_branpf1),'% ')\n",
    "print('after revision: vio_branang1: mean\\033[1;31m ',str(np.round(np.mean(vio_branang1.numpy()),2)), '\\033[0m% max', np.max(vio_branang1.numpy()),'% ', \\\n",
    "      'after revision: vio_branpf1: mean\\033[1;31m ',str(np.round(np.mean(vio_branpf1.numpy()),2)), '\\033[0m% max', np.max(vio_branpf1.numpy()),'% ')\n",
    "\n",
    "\n",
    "res_PQUL1 = np.array((np.mean(deltaPgL1[:,1]),np.min(deltaPgL1[:,1]),np.mean(deltaPgU1[:,1]),np.max(deltaPgU1[:,1]), \\\n",
    "                        np.mean(deltaQgL1[:,1]),np.min(deltaQgL1[:,1]),np.mean(deltaQgU1[:,1]),np.max(deltaQgU1[:,1])))\n",
    "\n",
    "if deltapf1.shape[1] > 2:\n",
    "    res_branpf1 = np.array((np.mean(deltapf1[:,1]),np.max(deltapf1[:,1]),np.mean(deltapf1[:,2]),np.max(deltapf1[:,2])))\n",
    "else:\n",
    "    res_branpf1 = np.array((np.mean(deltapf1[:,1]),np.max(deltapf1[:,1])))\n",
    "    \n",
    "print('after revision: res_branpf1 ', res_branpf1, 'res_PQUL1', res_PQUL1)   \n",
    "\n",
    "# time\n",
    "total_time_each_rev = total_time_each + revision_time_bus + revision_time_branch\n",
    "print('total_time_each', np.round(total_time_each,6))\n",
    "print('total_time_each_rev', np.round(total_time_each_rev,6))\n",
    "    \n",
    "optimality_loss = np.mean(np.abs((Pred_costtest-Real_costtest)/Real_costtest))*100\n",
    "print('optimality_loss',optimality_loss,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2972c2f-53c7-4909-b9f1-85e91f10189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_test:\n",
    "    # save results\n",
    "    res_tensor = torch.as_tensor([mae_Vmtest, mae_Vatest, np.mean(mre_cost), np.mean(mre_Pd), np.mean(mre_Qd), \\\n",
    "                    torch.mean(vio_PQg[:, 0]), torch.mean(vio_PQg[:, 1]), torch.mean(vio_branang), torch.mean(vio_branpf)]).reshape(1,-1)\n",
    "    res_tensor1 = torch.as_tensor([mae_Vmtest1, mae_Vatest1, np.mean(mre_cost1), np.mean(mre_Pd1), np.mean(mre_Qd1), \\\n",
    "                    torch.mean(vio_PQg1[:, 0]), torch.mean(vio_PQg1[:, 1]), torch.mean(vio_branang1), torch.mean(vio_branpf1)]).reshape(1,-1)\n",
    "    res = np.around(torch.cat((res_tensor, res_tensor1), axis = 1).numpy(), decimals=2)\n",
    "    mre_cost = np.array(mre_cost)\n",
    "    time = np.array([total_time_each, total_time_each_rev])\n",
    "    scipy.io.savemat(resultnm, \n",
    "                     mdict={'res': res,'res_PQUL': res_PQUL,'deltaPgL': deltaPgL,'deltaPgU': deltaPgU,'deltaQgL': deltaQgL, \\\n",
    "                            'deltaQgU': deltaQgU,'time_ZIB': time_ZIB,'time_NN': time_NN,'time_cal': time_cal,'total_time_each':total_time_each, \\\n",
    "                            'total_time_each_rev': total_time_each_rev,'revision_time_bus': revision_time_bus, \\\n",
    "                            'res_branpf':res_branpf,'mre_cost':mre_cost,'deltapf':deltapf,'deltapf1':deltapf1,\\\n",
    "                            'idx_train':idx_train,'idx_test':idx_test, \\\n",
    "                            'idx_his': idx_his, 'res_PQUL1': res_PQUL1,'deltaPgL1': deltaPgL1,'deltaPgU1': deltaPgU1,'deltaQgL1': deltaQgL1, \\\n",
    "                            'deltaQgU1': deltaQgU1, 'time': time,'optimality_loss':optimality_loss\n",
    "    })\n",
    "    \n",
    "else:\n",
    "        # save results\n",
    "    res_tensor = torch.as_tensor([mae_Vmtest, mae_Vatest, np.mean(mre_cost), np.mean(mre_Pd), np.mean(mre_Qd), \\\n",
    "                    torch.mean(vio_PQg[:, 0]), torch.mean(vio_PQg[:, 1]), torch.mean(vio_branang), torch.mean(vio_branpf)]).reshape(1,-1)\n",
    "    res_tensor1 = torch.as_tensor([mae_Vmtest1, mae_Vatest1, np.mean(mre_cost1), np.mean(mre_Pd1), np.mean(mre_Qd1), \\\n",
    "                    torch.mean(vio_PQg1[:, 0]), torch.mean(vio_PQg1[:, 1]), torch.mean(vio_branang1), torch.mean(vio_branpf1)]).reshape(1,-1)\n",
    "    res = np.around(torch.cat((res_tensor, res_tensor1), axis = 1).numpy(), decimals=2)\n",
    "    mre_cost = np.array(mre_cost)\n",
    "    time = np.array([total_time_each, total_time_each_rev, time_trian])\n",
    "    scipy.io.savemat(resultnm, \n",
    "                     mdict={'res': res,'res_PQUL': res_PQUL,'deltaPgL': deltaPgL,'deltaPgU': deltaPgU,'deltaQgL': deltaQgL, \\\n",
    "                            'deltaQgU': deltaQgU,'time_ZIB': time_ZIB,'time_NN': time_NN,'total_time_each':total_time_each, \\\n",
    "                            'total_time_each_rev': total_time_each_rev,'revision_time_bus': revision_time_bus, \\\n",
    "                            'res_branpf':res_branpf,'mre_cost':mre_cost,'deltapf':deltapf,'deltapf1':deltapf1,\\\n",
    "                            'idx_train':idx_train,'idx_test':idx_test, \\\n",
    "                            'idx_his': idx_his, 'res_PQUL1': res_PQUL1,'deltaPgL1': deltaPgL1,'deltaPgU1': deltaPgU1,'deltaQgL1': deltaQgL1, \\\n",
    "                            'deltaQgU1': deltaQgU1, 'time': time, 'time_trian': time_trian,'optimality_loss':optimality_loss\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2af74168-9de4-45fd-a78f-b15c4cd7f4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultnm ./result/Res_DeepOPFSB_krp_B30r0V1S32000Nhis3lr0.0001kc0.0kd100kg2000kv500ky1L32224bs50E45001Nt10000.mat\n"
     ]
    }
   ],
   "source": [
    "print('resultnm',resultnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bffe2fa-53b1-4de2-9dc6-a5cd65325257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetV(\n",
       "  (fc1): Linear(in_features=40, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=224, bias=True)\n",
       "  (fcbfend): Linear(in_features=224, out_features=47, bias=True)\n",
       "  (fcend): Linear(in_features=47, out_features=47, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254c24d-d285-4914-8efc-67aa50299543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7085e80-ac8b-4ffa-bba0-fcc6139e1f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
