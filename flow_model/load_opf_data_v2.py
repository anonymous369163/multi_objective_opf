"""
ä¿®æ”¹ç‰ˆæ•°æ®åŠ è½½å™¨ - ä¸ºæµæ¨¡å‹ä¼˜åŒ–
è®©æµæ¨¡å‹çœŸæ­£ä»anchoræµå‘target
"""
import torch
import numpy as np
import os


def filter_training_data(x_train, y_train, env, actor, max_violation=0.1):
    """è¿‡æ»¤æ‰ä¸¥é‡è¿åçº¦æŸçš„è®­ç»ƒæ ·æœ¬"""
    print("\næ•°æ®æ¸…æ´—ä¸­...")

    # åˆ¤æ–­actoræ¨¡å‹æ‰€åœ¨è®¾å¤‡ï¼Œå¹¶å°†è¾“å…¥æ•°æ®ç§»åˆ°ç›¸åŒè®¾å¤‡
    device = next(actor.parameters()).device if hasattr(actor, 'parameters') else torch.device('cpu')
    x_train_device = x_train.to(device) if isinstance(x_train, torch.Tensor) else torch.tensor(x_train, device=device)
    y_train_device = y_train.to(device) if isinstance(y_train, torch.Tensor) else torch.tensor(y_train, device=device)
    
    output_dim = y_train_device.shape[1]
    vm = y_train_device[:, :output_dim//2]
    va = y_train_device[:, output_dim//2:]

    with torch.no_grad():
        violations = actor.compute_constraint_loss(
            vm, va, x_train_device, env, reduction='none'
        )

    # åªä¿ç•™çº¦æŸè¿åå°äºé˜ˆå€¼çš„æ ·æœ¬
    valid_mask = violations < max_violation

    x_train_filtered = x_train_device[valid_mask]
    y_train_filtered = y_train_device[valid_mask]

    # è‹¥æ•°æ®åŸæœ¬åœ¨CPUä¸Šï¼Œåˆ™è½¬å›CPUï¼Œé¿å…æ½œåœ¨ç¯å¢ƒä¸ä¸€è‡´
    x_train_filtered = x_train_filtered.cpu()
    y_train_filtered = y_train_filtered.cpu()

    print(f"åŸå§‹æ ·æœ¬æ•°: {len(x_train)}")
    print(f"è¿‡æ»¤åæ ·æœ¬æ•°: {len(x_train_filtered)}")
    print(f"ä¿ç•™ç‡: {len(x_train_filtered)/len(x_train)*100:.1f}%")

    return x_train_filtered, y_train_filtered


def create_toy_dataset_with_clustering(data_path, n_clusters=30, train_samples=800, test_samples=200,
                                        random_seed=42, device='cpu', add_carbon_tax=False):
    """
    é€šè¿‡ K-means èšç±»åˆ›å»ºåˆ†å¸ƒç›¸ä¼¼çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    
    ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¥è‡ªç›¸åŒçš„èšç±»åˆ†å¸ƒï¼Œä½†æ ·æœ¬ä¸é‡å ã€‚
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        n_clusters: K-means èšç±»æ•°é‡
        train_samples: è®­ç»ƒé›†æ ·æœ¬æ•°é‡
        test_samples: æµ‹è¯•é›†æ ·æœ¬æ•°é‡
        random_seed: éšæœºç§å­
        device: è®¾å¤‡
        add_carbon_tax: æ˜¯å¦æ·»åŠ ç¢³ç¨ç‰¹å¾
        
    Returns:
        dict: {
            'x_train': è®­ç»ƒè¾“å…¥,
            'y_train': è®­ç»ƒç›®æ ‡,
            'x_test': æµ‹è¯•è¾“å…¥,
            'y_test': æµ‹è¯•ç›®æ ‡,
            'cluster_labels': èšç±»æ ‡ç­¾,
            'cluster_info': èšç±»ç»Ÿè®¡ä¿¡æ¯
        }
    """
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    
    print("=" * 60)
    print("åˆ›å»º Toy æ•°æ®é›† (K-means èšç±»é‡‡æ ·)")
    print("=" * 60)
    
    # åŠ è½½åŸå§‹æ•°æ®
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    data = np.load(data_path)
    train_inputs = data['train_inputs']      # [N, load_dim]
    train_targets = data['train_targets']    # [N, target_dim]
    
    n_total = train_inputs.shape[0]
    print(f"\nåŸå§‹æ•°æ®: {n_total} æ ·æœ¬")
    print(f"è¾“å…¥ç»´åº¦: {train_inputs.shape[1]}")
    print(f"è¾“å‡ºç»´åº¦: {train_targets.shape[1]}")
    
    # æ ‡å‡†åŒ–è¾“å…¥ç‰¹å¾ç”¨äºèšç±»
    np.random.seed(random_seed)
    scaler = StandardScaler()
    x_scaled = scaler.fit_transform(train_inputs)
    
    # K-means èšç±»
    print(f"\næ­£åœ¨è¿›è¡Œ K-means èšç±» (k={n_clusters})...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed, n_init=10)
    cluster_labels = kmeans.fit_predict(x_scaled)
    
    # ç»Ÿè®¡æ¯ä¸ªç°‡çš„æ ·æœ¬æ•°
    cluster_counts = np.bincount(cluster_labels, minlength=n_clusters)
    print(f"èšç±»å®Œæˆ:")
    print(f"  - ç°‡å¤§å°èŒƒå›´: [{cluster_counts.min()}, {cluster_counts.max()}]")
    print(f"  - ç°‡å¹³å‡å¤§å°: {cluster_counts.mean():.1f}")
    
    # è®¡ç®—æ¯ä¸ªç°‡åº”é‡‡æ ·çš„è®­ç»ƒ/æµ‹è¯•æ ·æœ¬æ•°
    total_samples = train_samples + test_samples
    train_ratio = train_samples / total_samples
    
    # ä»æ¯ä¸ªç°‡æŒ‰æ¯”ä¾‹é‡‡æ ·
    train_indices = []
    test_indices = []
    
    for cluster_id in range(n_clusters):
        # è·å–è¯¥ç°‡çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
        cluster_mask = (cluster_labels == cluster_id)
        cluster_indices = np.where(cluster_mask)[0]
        n_cluster = len(cluster_indices)
        
        if n_cluster == 0:
            continue
        
        # è®¡ç®—è¯¥ç°‡åº”é‡‡æ ·çš„æ•°é‡ï¼ˆæŒ‰ç°‡å¤§å°æ¯”ä¾‹ï¼‰
        cluster_ratio = n_cluster / n_total
        n_train_from_cluster = max(1, int(train_samples * cluster_ratio))
        n_test_from_cluster = max(1, int(test_samples * cluster_ratio))
        
        # ç¡®ä¿ä¸è¶…è¿‡ç°‡å¤§å°
        n_total_from_cluster = n_train_from_cluster + n_test_from_cluster
        if n_total_from_cluster > n_cluster:
            scale = n_cluster / n_total_from_cluster
            n_train_from_cluster = max(1, int(n_train_from_cluster * scale))
            n_test_from_cluster = max(1, int(n_test_from_cluster * scale))
        
        # éšæœºæ‰“ä¹±å¹¶åˆ†é…
        np.random.shuffle(cluster_indices)
        train_indices.extend(cluster_indices[:n_train_from_cluster])
        test_indices.extend(cluster_indices[n_train_from_cluster:n_train_from_cluster + n_test_from_cluster])
    
    train_indices = np.array(train_indices)
    test_indices = np.array(test_indices)
    
    # å¦‚æœæ ·æœ¬æ•°ä¸è¶³ï¼Œä»å‰©ä½™æ ·æœ¬ä¸­è¡¥å……
    all_used = set(train_indices) | set(test_indices)
    remaining = np.array([i for i in range(n_total) if i not in all_used])
    
    if len(train_indices) < train_samples and len(remaining) > 0:
        n_need = min(train_samples - len(train_indices), len(remaining))
        np.random.shuffle(remaining)
        train_indices = np.concatenate([train_indices, remaining[:n_need]])
        remaining = remaining[n_need:]
    
    if len(test_indices) < test_samples and len(remaining) > 0:
        n_need = min(test_samples - len(test_indices), len(remaining))
        test_indices = np.concatenate([test_indices, remaining[:n_need]])
    
    # æœ€ç»ˆæ‰“ä¹±é¡ºåº
    np.random.shuffle(train_indices)
    np.random.shuffle(test_indices)
    
    # æå–æ•°æ®
    x_train = train_inputs[train_indices]
    y_train = train_targets[train_indices]
    x_test = train_inputs[test_indices]
    y_test = train_targets[test_indices]
    
    print(f"\nToy æ•°æ®é›†åˆ›å»ºå®Œæˆ:")
    print(f"  - è®­ç»ƒé›†: {len(x_train)} æ ·æœ¬")
    print(f"  - æµ‹è¯•é›†: {len(x_test)} æ ·æœ¬")
    print(f"  - è®­ç»ƒ/æµ‹è¯•æ¯”ä¾‹: {len(x_train)/(len(x_train)+len(x_test))*100:.1f}%/{len(x_test)/(len(x_train)+len(x_test))*100:.1f}%")
    
    # éªŒè¯åˆ†å¸ƒç›¸ä¼¼æ€§
    train_cluster_dist = np.bincount(cluster_labels[train_indices], minlength=n_clusters) / len(train_indices)
    test_cluster_dist = np.bincount(cluster_labels[test_indices], minlength=n_clusters) / len(test_indices)
    
    # è®¡ç®—åˆ†å¸ƒå·®å¼‚ (Jensen-Shannon divergence çš„ç®€åŒ–ç‰ˆ)
    dist_diff = np.abs(train_cluster_dist - test_cluster_dist).mean()
    print(f"  - è®­ç»ƒ/æµ‹è¯•åˆ†å¸ƒå·®å¼‚: {dist_diff:.4f} (è¶Šå°è¶Šå¥½)")
    
    # è½¬æ¢ä¸º tensor
    x_train = torch.as_tensor(x_train, dtype=torch.float32, device=device)
    y_train = torch.as_tensor(y_train, dtype=torch.float32, device=device)
    x_test = torch.as_tensor(x_test, dtype=torch.float32, device=device)
    y_test = torch.as_tensor(y_test, dtype=torch.float32, device=device)
    
    print("=" * 60)
    
    return {
        'x_train': x_train,
        'y_train': y_train,
        'x_test': x_test,
        'y_test': y_test,
        'train_indices': train_indices,
        'test_indices': test_indices,
        'cluster_labels': cluster_labels,
        'cluster_info': {
            'n_clusters': n_clusters,
            'cluster_counts': cluster_counts,
            'train_cluster_dist': train_cluster_dist,
            'test_cluster_dist': test_cluster_dist,
            'dist_diff': dist_diff
        }
    }


class OPF_Flow_Dataset_V2:
    """
    æ”¹è¿›çš„æ•°æ®é›†ç±»ï¼Œä¸“é—¨ä¸ºä»anchoråˆ°targetçš„æµæ¨¡å‹è®¾è®¡
    
    æ•°æ®æ ¼å¼ï¼š
    - x: [è´Ÿè·c, ç¢³ç¨Î»]  (ä¸åŒ…å«anchorï¼Œå› ä¸ºanchoræ˜¯æµçš„èµ·ç‚¹)
    - y: [ç›®æ ‡target]
    - y_anchor: å•ç‹¬ä¿å­˜ï¼Œç”¨ä½œæµæ¨¡å‹çš„x_0
    
    æ”¹è¿›ï¼šåœ¨åŠ è½½æ—¶è‡ªåŠ¨åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç¡®ä¿éªŒè¯æ¨¡å‹æ³›åŒ–èƒ½åŠ›
    """
    
    def __init__(self, data_path, device='cpu', test_ratio=0.2, random_seed=42, add_carbon_tax=True, single_target=False):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            device: è®¾å¤‡ ('cpu' æˆ– 'cuda')
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ï¼Œé»˜è®¤0.2 (å³20%æµ‹è¯•ï¼Œ80%è®­ç»ƒ)
            random_seed: éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°æ€§
        """
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        print(f"æ­£åœ¨åŠ è½½æ•°æ®: {data_path}")
        data = np.load(data_path)
        self.single_target = single_target
        
        # åŠ è½½å„ä¸ªæ•°æ®å­—æ®µ
        train_inputs = data['train_inputs']      # shape: [N, load_dim]
        train_targets = data['train_targets']    # shape: [N, target_dim]
        if not single_target:
            preferences = data['preferences']        # shape: [N, 1] æˆ– [N,]
            y_anchors = data['y_anchors']           # shape: [N, target_dim]
            actions = data['actions']                # shape: [N, output_dim]
        
        # ç¡®ä¿preferencesæ˜¯2Dæ•°ç»„
        if not single_target and preferences.ndim == 1:
            preferences = preferences.reshape(-1, 1)
        
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶:")
        print(f"  - train_inputs (è´Ÿè·c): {train_inputs.shape}")
        print(f"  - train_targets (ç›®æ ‡y): {train_targets.shape}")
        if not single_target:
            print(f"  - preferences (ç¢³ç¨ç‡): {preferences.shape}")
            print(f"  - y_anchors (é”šç‚¹): {y_anchors.shape}")
            print(f"  - actions (åŠ¨ä½œ): {actions.shape}")
        
        # V2ç‰ˆæœ¬: xåªåŒ…å«[è´Ÿè·c, ç¢³ç¨Î»]ï¼Œä¸åŒ…å«anchor
        # anchorä½œä¸ºæµçš„èµ·ç‚¹å•ç‹¬å¤„ç†
        if add_carbon_tax:
            x_combined = np.concatenate([
                train_inputs,    # è´Ÿè·
                preferences,     # ç¢³ç¨ç‡ï¼ˆç›®æ ‡æƒé‡ï¼‰
            ], axis=1)
        else:
            x_combined = train_inputs 
        
        # è¾“å‡ºå°±æ˜¯ç›®æ ‡å†³ç­–å˜é‡
        y_combined = train_targets
        
        # ===== æ–°å¢ï¼šè‡ªåŠ¨åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† =====
        np.random.seed(random_seed)  # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°
        n_samples = x_combined.shape[0]
        indices = np.random.permutation(n_samples)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        n_test = int(n_samples * test_ratio)
        n_train = n_samples - n_test
        
        test_indices = indices[:n_test]
        train_indices = indices[n_test:]
        
        print(f"\næ•°æ®åˆ†å‰²:")
        print(f"  - æ€»æ ·æœ¬æ•°: {n_samples}")
        print(f"  - è®­ç»ƒé›†: {n_train} ({100*(1-test_ratio):.0f}%)")
        print(f"  - æµ‹è¯•é›†: {n_test} ({100*test_ratio:.0f}%)")
        print(f"  - éšæœºç§å­: {random_seed}")
        
        # åˆ†å‰²æ•°æ®
        x_train_split = x_combined[train_indices]
        y_train_split = y_combined[train_indices]
        if not single_target:
            y_anchor_train = y_anchors[train_indices]
        
        x_test_split = x_combined[test_indices]
        y_test_split = y_combined[test_indices]
        if not single_target:
            y_anchor_test = y_anchors[test_indices]
        
        # è½¬æ¢ä¸ºtorch tensor - è®­ç»ƒé›†
        self.x_train = torch.as_tensor(x_train_split, dtype=torch.float32)
        self.y_train = torch.as_tensor(y_train_split, dtype=torch.float32)
        if not single_target:
            self.y_anchor_train = torch.as_tensor(y_anchor_train, dtype=torch.float32)
        
        # è½¬æ¢ä¸ºtorch tensor - æµ‹è¯•é›†
        self.x_test = torch.as_tensor(x_test_split, dtype=torch.float32)
        self.y_test = torch.as_tensor(y_test_split, dtype=torch.float32)
        if not single_target:
            self.y_anchor_test = torch.as_tensor(y_anchor_test, dtype=torch.float32)
        
        # ä¿å­˜åŸå§‹æ•°æ®ï¼ˆç”¨äºåˆ†æï¼‰- åˆ†å‰²åçš„
        self.train_inputs = train_inputs[train_indices]
        self.train_targets = train_targets[train_indices]
        if not single_target:
            self.preferences_train = preferences[train_indices]
            self.y_anchors_train = y_anchors[train_indices]
            self.actions_train = actions[train_indices]
        
        self.test_inputs = train_inputs[test_indices]
        self.test_targets = train_targets[test_indices]
        if not single_target:
            self.preferences_test = preferences[test_indices]
            self.y_anchors_test = y_anchors[test_indices]
            self.actions_test = actions[test_indices]

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(device)
        self.analyze_data()
        
        # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    def analyze_data(self):
        self.num_train_samples = self.x_train.shape[0]
        self.num_test_samples = self.x_test.shape[0]
        self.num_samples = self.num_train_samples  # ä¿æŒå‘åå…¼å®¹
        self.input_dim = self.x_train.shape[1]
        self.output_dim = self.y_train.shape[1]
        self.load_dim = self.train_inputs.shape[1]
        self.target_dim = self.train_targets.shape[1]
        
        print(f"\næ•°æ®é›†æ„å»ºå®Œæˆ (ä¸ºæµæ¨¡å‹ä¼˜åŒ– + è®­ç»ƒ/æµ‹è¯•åˆ†å‰²):")
        print(f"  - è®­ç»ƒæ ·æœ¬: {self.num_train_samples}")
        print(f"  - æµ‹è¯•æ ·æœ¬: {self.num_test_samples}")
        print(f"  - è¾“å…¥ç»´åº¦: {self.input_dim} (è´Ÿè·:{self.load_dim} + åå¥½:1)")
        print(f"  - è¾“å‡ºç»´åº¦: {self.output_dim}")
        if not self.single_target:
            print(f"  - åå¥½ç»´åº¦: {self.preferences_train.shape[1]}")
            print(f"  - é”šç‚¹ç»´åº¦: {self.y_anchor_train.shape[1]}")
            print(f"  - åŠ¨ä½œç»´åº¦: {self.actions_train.shape[1]}") 
        print(f"  - è®¾å¤‡: {self.device}")
        
    def to(self, device):
        """å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        self.device = torch.device(device)
        # ç§»åŠ¨è®­ç»ƒé›†
        self.x_train = self.x_train.to(self.device)
        self.y_train = self.y_train.to(self.device)
        self.y_anchor_train = self.y_anchor_train.to(self.device)
        # ç§»åŠ¨æµ‹è¯•é›†
        self.x_test = self.x_test.to(self.device)
        self.y_test = self.y_test.to(self.device)
        self.y_anchor_test = self.y_anchor_test.to(self.device)
        return self
    
    def get_data_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'num_train_samples': self.num_train_samples,
            'num_test_samples': self.num_test_samples,
            'total_samples': self.num_train_samples + self.num_test_samples,
            'input_dim': self.input_dim,
            'output_dim': self.output_dim,
            'load_dim': self.load_dim,
            'target_dim': self.target_dim,
            'train_preference_range': (self.preferences_train.min(), self.preferences_train.max()),
            'test_preference_range': (self.preferences_test.min(), self.preferences_test.max()),
        }
    
    def get_train_data(self):
        """è·å–è®­ç»ƒé›†æ•°æ®"""
        return self.x_train, self.y_train, self.y_anchor_train
    
    def get_test_data(self):
        """è·å–æµ‹è¯•é›†æ•°æ®"""
        return self.x_test, self.y_test, self.y_anchor_test


class OPF_Flow_Dataset_Grouped:
    """
    åˆ†ç»„æ•°æ®é›†ç±»ï¼šå°†åŒä¸€è´Ÿè·åœºæ™¯ä¸‹ä¸åŒç¢³ç¨ç‡(åå¥½)çš„æ ·æœ¬ç»„ç»‡åœ¨ä¸€èµ·
    
    è¿™ä¸ªç±»ä¸“é—¨ä¸ºè®¾è®¡å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°è€Œè®¾è®¡ï¼Œä½¿å¾—åœ¨è®­ç»ƒæ—¶ï¼š
    - ä¸€ä¸ªbatchå†…åŒ…å«åŒä¸€è´Ÿè·åœºæ™¯ä¸‹å¤šä¸ªåå¥½çš„æœ€ä¼˜è§£
    - å¯ä»¥è®¡ç®—åŒä¸€åœºæ™¯ä¸‹ä¸åŒåå¥½è§£ä¹‹é—´çš„å…³ç³»
    - æœ‰åŠ©äºæ¨¡å‹å­¦ä¹ åå¥½å¯¹è§£çš„å½±å“è§„å¾‹
    
    æ•°æ®ç»„ç»‡ï¼š
    - è¯†åˆ«ç‹¬ç‰¹çš„è´Ÿè·åœºæ™¯
    - æ¯ä¸ªåœºæ™¯åŒ…å«å¤šä¸ª(preference, target, anchor)ä¸‰å…ƒç»„
    - æä¾›åœºæ™¯çº§åˆ«çš„æ‰¹æ¬¡é‡‡æ ·
    """
    
    def __init__(self, data_path, device='cpu', test_ratio=0.2, random_seed=42, 
                 add_carbon_tax=True, scenario_test_ratio=0.2):
        """
        åˆå§‹åŒ–åˆ†ç»„æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            device: è®¾å¤‡ ('cpu' æˆ– 'cuda')
            test_ratio: æœªä½¿ç”¨ï¼ˆä¿æŒAPIå…¼å®¹ï¼‰
            random_seed: éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°æ€§
            add_carbon_tax: æ˜¯å¦å°†ç¢³ç¨åŠ å…¥è¾“å…¥ç‰¹å¾
            scenario_test_ratio: åœºæ™¯çº§åˆ«çš„æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆæŒ‰åœºæ™¯åˆ’åˆ†ï¼Œéæ ·æœ¬ï¼‰
        """
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        print(f"=" * 70)
        print(f"æ­£åœ¨åŠ è½½åˆ†ç»„æ•°æ®é›†: {data_path}")
        print(f"=" * 70)
        
        data = np.load(data_path)
        
        # åŠ è½½å„ä¸ªæ•°æ®å­—æ®µ
        train_inputs = data['train_inputs']      # shape: [N, load_dim]
        train_targets = data['train_targets']    # shape: [N, target_dim]
        preferences = data['preferences']        # shape: [N, 1] æˆ– [N,]
        y_anchors = data['y_anchors']           # shape: [N, target_dim]
        
        # ç¡®ä¿preferencesæ˜¯2Dæ•°ç»„
        if preferences.ndim == 1:
            preferences = preferences.reshape(-1, 1)
        
        print(f"\nåŸå§‹æ•°æ®å½¢çŠ¶:")
        print(f"  - train_inputs (è´Ÿè·c): {train_inputs.shape}")
        print(f"  - train_targets (ç›®æ ‡y): {train_targets.shape}")
        print(f"  - preferences (ç¢³ç¨ç‡): {preferences.shape}")
        print(f"  - y_anchors (é”šç‚¹): {y_anchors.shape}")
        
        # ===== æ ¸å¿ƒï¼šè¯†åˆ«å’Œåˆ†ç»„è´Ÿè·åœºæ™¯ =====
        print(f"\næ­£åœ¨åˆ†æè´Ÿè·åœºæ™¯...")
        self._group_by_scenario(train_inputs, train_targets, preferences, y_anchors)
        
        # ===== åœºæ™¯çº§åˆ«çš„è®­ç»ƒ/æµ‹è¯•åˆ†å‰² =====
        print(f"\næ­£åœ¨è¿›è¡Œåœºæ™¯çº§åˆ«çš„æ•°æ®åˆ†å‰²...")
        np.random.seed(random_seed)
        n_scenarios = len(self.scenario_indices)
        scenario_ids = np.arange(n_scenarios)
        np.random.shuffle(scenario_ids)
        
        n_test_scenarios = int(n_scenarios * scenario_test_ratio)
        n_train_scenarios = n_scenarios - n_test_scenarios
        
        self.train_scenario_ids = scenario_ids[n_test_scenarios:]
        self.test_scenario_ids = scenario_ids[:n_test_scenarios]
        
        print(f"  - æ€»åœºæ™¯æ•°: {n_scenarios}")
        print(f"  - è®­ç»ƒåœºæ™¯: {n_train_scenarios} ({100*(1-scenario_test_ratio):.0f}%)")
        print(f"  - æµ‹è¯•åœºæ™¯: {n_test_scenarios} ({100*scenario_test_ratio:.0f}%)")
        
        # ç»Ÿè®¡è®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬æ•°
        n_train_samples = sum(len(self.scenario_indices[sid]) for sid in self.train_scenario_ids)
        n_test_samples = sum(len(self.scenario_indices[sid]) for sid in self.test_scenario_ids)
        print(f"  - è®­ç»ƒæ ·æœ¬æ€»æ•°: {n_train_samples}")
        print(f"  - æµ‹è¯•æ ·æœ¬æ€»æ•°: {n_test_samples}")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        self.train_inputs_raw = train_inputs
        self.train_targets_raw = train_targets
        self.preferences_raw = preferences
        self.y_anchors_raw = y_anchors
        
        self.add_carbon_tax = add_carbon_tax
        self.device = torch.device(device)
        self.load_dim = train_inputs.shape[1]
        self.target_dim = train_targets.shape[1]
        
        print(f"\n" + "=" * 70)
        print(f"åˆ†ç»„æ•°æ®é›†æ„å»ºå®Œæˆï¼")
        print(f"=" * 70)
    
    def _group_by_scenario(self, train_inputs, train_targets, preferences, y_anchors):
        """
        æ ¹æ®è´Ÿè·åœºæ™¯åˆ†ç»„æ•°æ®
        
        ä½¿ç”¨å“ˆå¸Œæ¥è¯†åˆ«ç›¸åŒçš„è´Ÿè·åœºæ™¯ï¼ˆå®¹å¿æ•°å€¼è¯¯å·®ï¼‰
        """
        from collections import defaultdict
        
        # ç”¨äºå­˜å‚¨åœºæ™¯çš„å­—å…¸ï¼šscenario_hash -> list of sample indices
        scenario_dict = defaultdict(list)
        
        # å°†æ¯ä¸ªè´Ÿè·åœºæ™¯è½¬æ¢ä¸ºå“ˆå¸Œå€¼ï¼ˆå››èˆäº”å…¥ä»¥å®¹å¿æµ®ç‚¹è¯¯å·®ï¼‰
        n_samples = train_inputs.shape[0]
        for i in range(n_samples):
            # å°†è´Ÿè·å‘é‡å››èˆäº”å…¥åˆ°å°æ•°ç‚¹å6ä½å¹¶è½¬ä¸ºå…ƒç»„ä½œä¸ºkey
            load_key = tuple(np.round(train_inputs[i], decimals=6))
            scenario_dict[load_key].append(i)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        self.unique_scenarios = []  # å­˜å‚¨ç‹¬ç‰¹çš„è´Ÿè·åœºæ™¯
        self.scenario_indices = []  # æ¯ä¸ªåœºæ™¯å¯¹åº”çš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        self.scenario_preference_counts = []  # æ¯ä¸ªåœºæ™¯æœ‰å¤šå°‘ä¸ªåå¥½
        
        for load_key, indices in scenario_dict.items():
            self.unique_scenarios.append(np.array(load_key))
            self.scenario_indices.append(indices)
            self.scenario_preference_counts.append(len(indices))
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.unique_scenarios = np.array(self.unique_scenarios)
        
        # ç»Ÿè®¡ä¿¡æ¯
        n_unique_scenarios = len(self.unique_scenarios)
        min_prefs = min(self.scenario_preference_counts)
        max_prefs = max(self.scenario_preference_counts)
        avg_prefs = np.mean(self.scenario_preference_counts)
        
        print(f"  - è¯†åˆ«å‡º {n_unique_scenarios} ä¸ªç‹¬ç‰¹çš„è´Ÿè·åœºæ™¯")
        print(f"  - æ¯ä¸ªåœºæ™¯çš„åå¥½æ•°é‡: æœ€å°={min_prefs}, æœ€å¤§={max_prefs}, å¹³å‡={avg_prefs:.1f}")
        
        # æ˜¾ç¤ºåœºæ™¯åˆ†å¸ƒç›´æ–¹å›¾
        pref_counts = np.array(self.scenario_preference_counts)
        unique_counts = np.unique(pref_counts)
        print(f"\n  åœºæ™¯-åå¥½åˆ†å¸ƒ:")
        for count in unique_counts:
            n_scenarios_with_count = np.sum(pref_counts == count)
            print(f"    - {count} ä¸ªåå¥½: {n_scenarios_with_count} ä¸ªåœºæ™¯")
    
    def get_scenario_batch(self, scenario_ids, split='train'):
        """
        è·å–æŒ‡å®šåœºæ™¯çš„æ‰¹æ¬¡æ•°æ®
        
        Args:
            scenario_ids: åœºæ™¯IDåˆ—è¡¨æˆ–å•ä¸ªåœºæ™¯ID
            split: 'train' æˆ– 'test'ï¼ˆæš‚æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥ä¾¿æ‰©å±•ï¼‰
            
        Returns:
            x_batch: [total_samples, input_dim] è¾“å…¥ç‰¹å¾
            y_batch: [total_samples, target_dim] ç›®æ ‡è¾“å‡º
            y_anchor_batch: [total_samples, target_dim] é”šç‚¹
            scenario_masks: listï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªboolæ•°ç»„ï¼Œæ ‡è¯†å±äºå“ªä¸ªåœºæ™¯
        """
        if isinstance(scenario_ids, int):
            scenario_ids = [scenario_ids]
        
        x_list = []
        y_list = []
        y_anchor_list = []
        scenario_masks = []
        
        current_idx = 0
        for sid in scenario_ids:
            indices = self.scenario_indices[sid]
            n_samples_in_scenario = len(indices)
            
            # æå–è¯¥åœºæ™¯çš„æ‰€æœ‰æ ·æœ¬
            loads = self.train_inputs_raw[indices]
            targets = self.train_targets_raw[indices]
            prefs = self.preferences_raw[indices]
            anchors = self.y_anchors_raw[indices]
            
            # æ„å»ºè¾“å…¥ x
            if self.add_carbon_tax:
                x = np.concatenate([loads, prefs], axis=1)
            else:
                x = loads
            
            x_list.append(x)
            y_list.append(targets)
            y_anchor_list.append(anchors)
            
            # è®°å½•è¯¥åœºæ™¯çš„æ ·æœ¬æ©ç 
            mask = np.zeros(current_idx + n_samples_in_scenario + 
                          sum(len(self.scenario_indices[s]) for s in scenario_ids[len(scenario_masks)+1:]), 
                          dtype=bool)
            mask[current_idx:current_idx + n_samples_in_scenario] = True
            scenario_masks.append(mask[:current_idx + n_samples_in_scenario])
            current_idx += n_samples_in_scenario
        
        # åˆå¹¶æ‰€æœ‰åœºæ™¯çš„æ•°æ®
        x_batch = np.concatenate(x_list, axis=0)
        y_batch = np.concatenate(y_list, axis=0)
        y_anchor_batch = np.concatenate(y_anchor_list, axis=0)
        
        # è½¬æ¢ä¸ºtensor
        x_batch = torch.as_tensor(x_batch, dtype=torch.float32, device=self.device)
        y_batch = torch.as_tensor(y_batch, dtype=torch.float32, device=self.device)
        y_anchor_batch = torch.as_tensor(y_anchor_batch, dtype=torch.float32, device=self.device)
        
        # ä¿®æ­£scenario_masks
        scenario_masks_corrected = []
        start = 0
        for sid in scenario_ids:
            n = len(self.scenario_indices[sid])
            mask = torch.zeros(len(x_batch), dtype=torch.bool, device=self.device)
            mask[start:start+n] = True
            scenario_masks_corrected.append(mask)
            start += n
        
        return x_batch, y_batch, y_anchor_batch, scenario_masks_corrected
    
    def create_scenario_batches(self, batch_size=32, split='train', shuffle=True):
        """
        åˆ›å»ºåœºæ™¯çº§åˆ«çš„æ‰¹æ¬¡è¿­ä»£å™¨
        
        Args:
            batch_size: æ¯ä¸ªbatchåŒ…å«çš„åœºæ™¯æ•°é‡ï¼ˆä¸æ˜¯æ ·æœ¬æ•°ï¼‰
            split: 'train' æˆ– 'test'
            shuffle: æ˜¯å¦æ‰“ä¹±åœºæ™¯é¡ºåº
            
        Yields:
            æ¯æ¬¡yieldä¸€ä¸ªbatchçš„æ•°æ®ï¼š(x_batch, y_batch, y_anchor_batch, scenario_masks)
        """
        if split == 'train':
            scenario_ids = self.train_scenario_ids.copy()
        else:
            scenario_ids = self.test_scenario_ids.copy()
        
        if shuffle:
            np.random.shuffle(scenario_ids)
        
        # æŒ‰batch_sizeåˆ†æ‰¹
        n_batches = (len(scenario_ids) + batch_size - 1) // batch_size
        
        for i in range(n_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(scenario_ids))
            batch_scenario_ids = scenario_ids[start_idx:end_idx]
            
            yield self.get_scenario_batch(batch_scenario_ids, split=split)
    
    def get_data_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        n_train_scenarios = len(self.train_scenario_ids)
        n_test_scenarios = len(self.test_scenario_ids)
        n_train_samples = sum(len(self.scenario_indices[sid]) for sid in self.train_scenario_ids)
        n_test_samples = sum(len(self.scenario_indices[sid]) for sid in self.test_scenario_ids)
        
        return {
            'n_unique_scenarios': len(self.unique_scenarios),
            'n_train_scenarios': n_train_scenarios,
            'n_test_scenarios': n_test_scenarios,
            'n_train_samples': n_train_samples,
            'n_test_samples': n_test_samples,
            'load_dim': self.load_dim,
            'target_dim': self.target_dim,
            'input_dim': self.load_dim + (1 if self.add_carbon_tax else 0),
            'preference_range': (self.preferences_raw.min(), self.preferences_raw.max()),
            'avg_preferences_per_scenario': np.mean(self.scenario_preference_counts),
            'min_preferences_per_scenario': min(self.scenario_preference_counts),
            'max_preferences_per_scenario': max(self.scenario_preference_counts),
        }
    
    def to(self, device):
        """å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        self.device = torch.device(device)
        return self


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import torch
    
    data_path = "saved_data/training_data_case118_40k_preferences.npz"
    
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    print("\n" + "="*80)
    print("æµ‹è¯• 1: OPF_Flow_Dataset_V2 (åŸå§‹ç‰ˆæœ¬)")
    print("="*80)
    
    try:
        # åŠ è½½æ•°æ®ï¼ˆé»˜è®¤20%æµ‹è¯•é›†ï¼Œ80%è®­ç»ƒé›†ï¼‰
        data = OPF_Flow_Dataset_V2(data_path, device=DEVICE, test_ratio=0.2, random_seed=42)
        
        # æ‰“å°æ•°æ®ç»Ÿè®¡
        print("\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
        info = data.get_data_info()
        for key, value in info.items():
            print(f"  - {key}: {value}")
        
        print("\nè®­ç»ƒé›†æ•°æ®å½¢çŠ¶:")
        print(f"  - x_train: {data.x_train.shape}")
        print(f"  - y_train: {data.y_train.shape}")
        print(f"  - y_anchor_train: {data.y_anchor_train.shape}")
        
        print("\næµ‹è¯•é›†æ•°æ®å½¢çŠ¶:")
        print(f"  - x_test: {data.x_test.shape}")
        print(f"  - y_test: {data.y_test.shape}")
        print(f"  - y_anchor_test: {data.y_anchor_test.shape}")
        
        # è·å–æ•°æ®çš„ä¾¿æ·æ–¹æ³•
        x_train, y_train, y_anchor_train = data.get_train_data()
        x_test, y_test, y_anchor_test = data.get_test_data()
        
        print("\nâœ“ OPF_Flow_Dataset_V2 æµ‹è¯•æˆåŠŸï¼")
        
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}")
    
    print("\n" + "="*80)
    print("æµ‹è¯• 2: OPF_Flow_Dataset_Grouped (åˆ†ç»„ç‰ˆæœ¬)")
    print("="*80)
    
    try:
        # åŠ è½½åˆ†ç»„æ•°æ®é›†
        grouped_data = OPF_Flow_Dataset_Grouped(
            data_path, 
            device=DEVICE, 
            scenario_test_ratio=0.2, 
            random_seed=42,
            add_carbon_tax=True
        )
        
        # æ‰“å°æ•°æ®ç»Ÿè®¡
        print("\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
        info = grouped_data.get_data_info()
        for key, value in info.items():
            print(f"  - {key}: {value}")
        
        # æµ‹è¯•åœºæ™¯æ‰¹æ¬¡ç”Ÿæˆ
        print("\næµ‹è¯•åœºæ™¯æ‰¹æ¬¡ç”Ÿæˆ:")
        print("-" * 70)
        
        # è·å–ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡
        batch_iter = grouped_data.create_scenario_batches(batch_size=2, split='train', shuffle=False)
        x_batch, y_batch, y_anchor_batch, scenario_masks = next(batch_iter)
        
        print(f"æ‰¹æ¬¡æ•°æ®å½¢çŠ¶:")
        print(f"  - x_batch: {x_batch.shape}")
        print(f"  - y_batch: {y_batch.shape}")
        print(f"  - y_anchor_batch: {y_anchor_batch.shape}")
        print(f"  - åœºæ™¯æ•°: {len(scenario_masks)}")
        
        # æ˜¾ç¤ºæ¯ä¸ªåœºæ™¯çš„æ ·æœ¬æ•°
        print(f"\næ¯ä¸ªåœºæ™¯çš„æ ·æœ¬æ•°:")
        for i, mask in enumerate(scenario_masks):
            n_samples = mask.sum().item()
            print(f"  - åœºæ™¯ {i}: {n_samples} ä¸ªåå¥½æ ·æœ¬")
        
        # å±•ç¤ºå¦‚ä½•ä½¿ç”¨maskæ¥åˆ†ç¦»åœºæ™¯å†…çš„æ•°æ®
        print(f"\nç¤ºä¾‹ï¼šä½¿ç”¨maskæå–åœºæ™¯0çš„æ•°æ®:")
        scene0_x = x_batch[scenario_masks[0]]
        scene0_y = y_batch[scenario_masks[0]]
        scene0_prefs = scene0_x[:, -1]  # æœ€åä¸€åˆ—æ˜¯åå¥½
        print(f"  - åœºæ™¯0çš„è¾“å…¥å½¢çŠ¶: {scene0_x.shape}")
        print(f"  - åœºæ™¯0çš„åå¥½å€¼: {scene0_prefs.cpu().numpy()}")
        
        print("\nâœ“ OPF_Flow_Dataset_Grouped æµ‹è¯•æˆåŠŸï¼")
        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print("  1. ä½¿ç”¨ create_scenario_batches() è¿­ä»£è®­ç»ƒ/æµ‹è¯•æ•°æ®")
        print("  2. æ¯ä¸ªbatchåŒ…å«å¤šä¸ªåœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯æœ‰å¤šä¸ªåå¥½æ ·æœ¬")
        print("  3. ä½¿ç”¨ scenario_masks æ¥åŒºåˆ†ä¸åŒåœºæ™¯çš„æ ·æœ¬")
        print("  4. å¯ä»¥åŸºäºåœºæ™¯å†…çš„æ ·æœ¬è®¾è®¡å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°")
        print("     ä¾‹å¦‚ï¼šåŒä¸€åœºæ™¯ä¸‹ï¼Œé«˜ç¢³ç¨åº”è¯¥äº§ç”Ÿæ›´ä½ç¢³çš„è§£")
        
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}")
    except Exception as e:
        print(f"è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

